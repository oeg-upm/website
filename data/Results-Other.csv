Resultados,id,,Descripcion,Description,Enlace,Figuras,Informacion adicional
Publicaciones,publicaciones,,,,http://oa.upm.es/cgi/search/advanced?screen=Public%3A%3AEPrintSearch&title_merge=ALL&title=&creators_name_merge=ALL&creators_name=&contributors_name_merge=ALL&contributors_name=&_fulltext__merge=ALL&_fulltext_=&abstract_merge=ALL&abstract=&keywords_merge=ALL&keywords=&subjects_merge=ALL&editors_name_merge=ALL&editors_name=&refereed=EITHER&publication_merge=ALL&publication=&event_title_merge=ALL&event_title=&note_merge=ALL&note=OEG&date=&satisfyall=ALL&order=-date%2Fcreators_name%2Ftitle&_action_search=Buscar,,
Ontologi­as,ontologia,,,,,,
Tecnologias y modelos,tecnologiasymodelos,CIDER-CL,"CIDER-CL es un sistema para el alineamiento monolingue y multilingüe entre ontologías. Para ello utiliza varias medidas semánticas como SoftTFIDF y Cross-Lingual Explicit Semantic Analysis. El cálculo de diferentes similitudes semánticas se combina mediante el uso de redes neuronales artificiales. CIDER-CL tiene dos modos de operar:
-Alineador de ontologías: toma dos ontologías en OWL como entrada y descubre sus equivalencias, que son ofrecidas como salida en formato RDF.
-Servicio de similitud semántica: toma dos entidades ontológicas como entrada y da el valor numérico de la similitud semática entre ellos como salida. ","CIDER-CL is a system for monolingual and multilingual alignment between ontologies. To do this, it uses various semantic measures such as SoftTFIDF and Cross-Lingual Explicit Semantic Analysis. The calculation of different semantic similarities is combined by using artificial neural networks. CIDER-CL has two modes of operation:
-Ontology aligner: takes two ontologies in OWL as input and discovers their equivalences, which are offered as output in RDF format.
-Semantic similarity service: it takes two ontological entities as input and gives the numerical value of the semantic similarity between them as output.",https://www.oeg-upm.net/files/cider-cl,,
Tecnologias y modelos,tecnologiasymodelos,geometry2rdf,geometry2rdf es una librería para generar ficheros RDF a partir de información geométrica (que puede estar disponible en GML o WKT). La manipulación del GML y WKT se realiza con GeoTools. La version actual de la librería trabaja con bases de datos geoespaciales y se basaa en Jena. geometry2rdf ha sido desarrollado por el equipo de GeoLinked Data (.es).,geometry2rdf is a library to generate RDF files from geometric information (which can be available in GML or WKT). Manipulation of GML and WKT is done with GeoTools. The current version of the library works with geospatial databases and is based on Jena. geometry2rdf has been developed by the GeoLinked Data (.es) team.,https://oeg.fi.upm.es/files/geometry2rdf/geometry2rdf_bin_0.0.3.zip,https://oeg.fi.upm.es/images/stories/tecnologias/geometry2rdf.png,"Quick start
Necesitas: Java 1.5 o superior en el path (comprueba con java -version si no estás seguro)
Que hacer:
-Descargar y extraer los archivos a un directorio de trabajo.
-La distribución actual viene con un un fichero de propiedades de ejemplo.   Este fichero puede usarse como el fichero base con el que puedes empezar a trabajar.
-Ejecuta geometry2rdf desde línea de comandos.
-Espera hasta que finalice, y comprueba el fichero rdf creado.
Trabajando con el fichero de configuración
La librería necesita un fichero de propiedades para su ejecución. El nombre por defecto del fichero es options.properties. Se proporciona un fichero de pruebas con ese nombre en la distribución para us uso. Si quieres usar un fichero diferente, cambia la ruta del fichero de propiedades en el fichero bat.                                                                                                                        Estructura del fichero de propiedades:
1. Input and output parameters: Directorio de trabajo y nombre del fichero de salida.
2. DDBB parameters: Todos los parámetros necesarios para la conexión con la BBDD se configuran en esta sección. También se configuran en esta sección el nombre del tipo de recursos que se van a crear.
3. Namespaces parameters: En esta sección se indican los namespaces y prefijos para los recursos generados y para la ontología usada.
4. Reference systems parameters: Geometry2rdf funciona para sistemas de referencia EPSG. Si tus datos gml no están en EPGS, se necesita una transformación entres tu sistema de referencia y un sistema de referencia EPSG. Para ello, utiliza los campos fields gmlSourceRS y gmlTargetRS. Si se require o se desea alguna tranformación más para los datos en el fichero RDF resultante, utilice sourceRS and targetRS.
5. Types parameters: En esta sección, se definen las URIs para los recursos Point, Linestring and Polygon. También se define la URI de la relación ""formBy"". Un recurso Linestring y Polygon están ""formBy"" Points.
6. Other parameters: En esta sección, se define el lenguaje por defecto de las etiquetas de cada recurso.                                       Ejemplos de ficheros RDF de salida.
Aquí puedes consultar unos ejemplos de ficheros RDF de salida:
-Ejemplo de un recurso de tipo Municipio con geometría formada por un único punto. https://oeg.fi.upm.es/files/geometry2rdf/Point.rdf
-Ejemplo de un recurso de tipo Balsa con una geometría de tipo Linestring. https://oeg.fi.upm.es/files/geometry2rdf/Linestring.rdf
-Ejemplo de un recurso de tipo Pozo con una geometría de tipo Polygon. https://oeg.fi.upm.es/files/geometry2rdf/Polygon.rdf"
Tecnologias y modelos,tecnologiasymodelos,gOntt,"gOntt es una herramienta para planificar y ejecutar proyectos de desarrollo cuyas principales características son las siguientes:
-Hace uso de plantillas orientadas a la planificación de desarrollos de ontologías. Estos desarrollos se basan en los escenarios propuestos por la NeOn Methodology.
-Genera las planificaciones de proyectos de desarrollo de ontologías en forma de diagramas Gantt.
-Informa a los desarrolladores de ontologías sobre cómo llevar a cabo un proceso o una actividad haciendo uso de guías metodológicas prescriptivas. También informa sobre las herramientas específicas de NeOn Toolkit que deben ser usadas.","gOntt is a tool for planning and executing development projects whose main features are the following:
-Makes use of templates oriented to the planning of ontology developments. These developments are based on the scenarios proposed by the NeOn Methodology.
-Generate the planning of ontology development projects in the form of Gantt charts.
-Informs ontology developers on how to carry out a process or an activity using prescriptive methodological guides. It also informs about the specific NeOn Toolkit tools that should be used.",http://neon-toolkit.org/wiki/Download,https://oeg.fi.upm.es/images/stories/tecnologias/gontt.png,"Funcionalidades de gOntt para la planificación de proyectos de desarrollo de ontologías.
gOntt ayuda a los desarrolladores de ontologías a decidir qué ciclo de vida es el más apropiado para construir sus ontologías (cascada o incremental-iterativo) así como qué procesos y actividades deben llevarse a cabo y en qué orden (por ejemplo, especificar los requisitos de la ontología antes de proceder a la transformar un recurso de conocimiento en una ontología). gOntt genera una representación gráfica de la planificación en forma de diagrama Gantt con los procesos y actividades requeridos, incluyendo las restricciones entre ellos. Las planificaciones de proyectos de desarrollo de ontologías pueden ser creadas desde cero (incluyendo procesos, actividades, fases y restricciones entre todos ellos) o de un modo guiado.
En el modo guiado, gOntt crea un plan preliminar para el desarrollo de la ontología en forma de:
-Plantillas que generan automáticamente la planificación inicial para proyectos de desarrollo de ontologías. Estas plantillas han sido construidas teniendo en cuenta la metodología NeOn. Las plantillas contienen un plan por defecto basado en las diferentes posibles combinaciones que se pueden dar entre modelos de ciclos de vida y procesos y actividades.
-Un simple asistente en dos pasos que contiene preguntas intuitivas que permiten al desarrollador de ontologías seleccionar el modelo de ciclo de vida de las ontologías y los procesos y actividades necesarias para su desarrollo. Para contestar a tales preguntas el desarrollador de ontologías ha de tener en cuenta los requisitos de la ontología y el tipo de recursos de conocimiento candidatos para ser reutilizados.
La principal salida de gOntt es el plan inicial para la construcción de la ontología en forma de diagrama Gantt que el desarrollador puede modificar con posterioridad ya sea: (a) incluyendo, modificando o borrando procesos, actividades y fases, (b) cambiando el orden y las dependencias entre procesos y actividades, e (c) incluyendo asignación de recursos y restricciones al plan. Esta funcionalidad de generar planes preliminares por defecto supone una gran ventaja sobre otras herramientas de planificación de proyectos.

Funcionalidades de gOntt para ayudar a la ejecución de proyectos
gOntt proporciona guías metodologías prescriptivas en forma de: (1) tarjetas que incluyen la definición de los procesos o actividades, sus objetivos, entradas, salidas, ejecutores de la acción y momento de la ejecución, y (2) un flujo de trabajo que describe cómo debe ser llevado a cabo el proceso o la actividad, así como sus entradas, salidas, tareas y actores involucrados. Además, gOntt proporciona un acceso directo y automatizado a las herramientas de NeOn Toolkit asociadas a cada proceso y actividad. Complementariamente, gOntt muestra una guía de comienzo rápido para cada herramienta lanzada.

Cómo utilizar gOntt
Para usar gOntt, en primer lugar, se debe instalar NeOn Toolkit (http://neon-toolkit.org/wiki/Download), y después, el plugin gOntt.En el siguiente enlace (http://www.neon-project.org/nw/Movie:_gOntt) se muestra un vídeo que muestra cómo usar gOntt."
Tecnologias y modelos,tecnologiasymodelos,Kyrie,"Kyrie es un sistema de reescritura de consultas que usa una ontología para reescribir una consulta Datalog en otra consulta Datalog que captura el conocimiento de la ontología. La consulta reescrita obtiene extensionalmente de la fuente de datos los resultados (certain answers) que están implicados tanto extensionalmente como intensionalmente por la consulta original. En el caso de las consultas no recursivas es posible desplegar la consulta Datalog en una unión de consultas conjuntivas, la cual puede ser posteriormente transformada a SQL u otros lenguajes, con otras herramientas como Morph.
Kyrie se inició como una derivación de REQUIEM. Los objetivos para este sistema han sido:
-mejorar los tiempos de reescritura
-producir consultas más cortas cuando sea posible, puesto que deberían suponer una carga computacional menor para los sistemas que aceptan las consultas reescritas
-mantener la lógica expresiva ELHIO para la descripción de la ontología
Estos objetivos han sido satisfechos como puede verse en la evaluación.","Kyrie is a query rewriting system that uses an ontology to rewrite a Datalog query into another Datalog query that captures knowledge of the ontology. The rewritten query gets extensionally from the data source the results (certain answers) that are both extensionally and intensionally implied by the original query. In the case of non-recursive queries, it is possible to display the Datalog query in a union of conjunctive queries, which can later be transformed to SQL or other languages, with other tools such as Morph.
Kyrie started out as an offshoot of REQUIEM. The objectives for this system have been:
-improve rewriting times
-produce shorter queries when possible, since they should put less computational load on systems that accept rewritten queries
-maintain the ELHIO expressive logic for the ontology description
These objectives have been satisfied as can be seen in the evaluation.",http://j.mp/benchmarkqueryrewriting,,"Las principales publicaciones referentes a este trabajo son las siguientes:
-Mora, José and Corcho, Oscar. ""Engineering optimisations in query rewriting for OBDA"" I-SEMANTICS, 2013, 4-6 de septiembre 2013, Graz, Austria.
-Mora, J. & Corcho, O. (2013). Towards a systematic benchmarking of ontology-based query rewriting systems. The 12th International Semantic Web Conference (ISWC2013) (p./pp. 369--384), 21-26 de octubre 2013, Sydney, Australia."
Tecnologias y modelos,tecnologiasymodelos,LabelTranslator,"LabelTranslator es un plug-in desarrollado dentro del proyecto NeOn. Este plug-in está implementado para la herramienta NeOn Toolkit y su función es la localización de los elementos , clases y propiedades, de una ontología en OWL creando un modelo repositorio lingüístico, llamado LIR por sus iniciales en inglés (Linguistic Information Repository).
El acceso multilingüe a las ontologías, hoy en día, se exige por parte de las instituciones de todo el mundo con un gran número de recursos disponibles en diferentes idiomas. Para resolver este problema, proponemos LabelTranslator, un plug-in que automáticamente localiza ontologías. LabelTranslator toma como entrada una ontología cuyas etiqueta se describen en un lenguaje natural fuente y obtiene la traducción más probable en lenguaje natural destino. En total, el plug-in soporta las siguientes funcionalidades:
-Obtiene la traducción más probable para cada etiqueta de la ontología. LabelTranslator se basa en dos módulos avanzados para esta tarea. El primer servicio, de traducción, obtiene automáticamente las diferentes traducciones posibles de una etiqueta de la ontología mediante el acceso a diferentes recursos lingüísticos. Este servicio también utiliza un método de composición con el fin de traducir las etiquetas compuestas (etiquetas formadas por varias palabras). El segundo módulo, el ranking de las traducciones, ordena las diferentes traducciones de acuerdo a la similitud con su contexto léxico y semántico. El método se basa en una medida de relación sobre la base de glosas a la ambigüedad de las traducciones. Esto se realiza mediante la comparación de los sentidos asociados a cada posible traducción y su contexto.
-Captura toda la información lingüística asociada con los conceptos utilizando un modelo lingüístico como repositorio (LIR). LIR, Repositorio de Información Lingüística, es un modelo portátil que se puede asociar a cualquiera de los términos de una ontología OWL a través de un meta-ontología OWL. Las clases principales que componen LIR (lexicalización, sentido de la definición, contexto de uso, notas y procedencia) se organizan alrededor de la clase LexicalEntry, que está vinculado a  cada término de la ontología (por medio de la relación hasLexicalEntry). El conjunto de conceptos LIR permite una completa descripción en lenguaje natural del término de la ontología al que está asociado. Además, por medio de las típicas relaciones léxicas ya sea en el mismo idioma (por ejemplo, hasSynonym) a través de idiomas (hasTranslation), LIR organiza la información lingüística en el mismo lenguaje natural y entre diferentes idiomas con el fin de proporcionar un conjunto multilingüe de información que permita la localización  de la ontología.
-Utiliza un mecanismo de sincronización para mantener la ontología y la información lingüística sincronizada. La adición de nuevos términos en la ontología o la supresión de términos existentes se controla mediante el avanzado seguimiento de cambios (basado en los recursos Delta1) utilizado en NeOn Toolkit. Este mecanismo es capaz de captar los cambios aun cuando ontológicamente hayan cambiado su posición en el modelo de la ontología. Con la adopción de esta función, LabelTranslator puede identificar con precisión el conjunto mínimo de cambios necesarios para ajustar la estructura del modelo lingüístico, un paso crítico para asegurar que un cambio importante se haga  en la ontología localizado.
Para descargar LabelTranslator, es necesario disponer de NeOn Toolkit que se puede descargar desde la página principal del proyecto NeOn Toolkit. Una vez descargada la herramienta e instalada, mediante el update site que proporciona la herramienta, podrá descargarse el plug-in LabelTranslator. La documentación de uso está disponible en la herramienta una vez descargado el plug-in e instalado.","LabelTranslator is a plug-in developed within the NeOn project. This plug-in is implemented for the NeOn Toolkit tool and its function is to locate the elements, classes and properties of an ontology in OWL creating a linguistic repository model, called LIR by its initials in English (Linguistic Information Repository).
Multilingual access to ontologies, today, is required by institutions around the world with a large number of resources available in different languages. To solve this problem, we propose LabelTranslator, a plug-in that automatically locates ontologies. LabelTranslator takes as input an ontology whose labels are described in a source natural language and obtains the most probable translation in target natural language. In total, the plug-in supports the following functionalities:
-Get the most probable translation for each ontology tag. LabelTranslator relies on two advanced modules for this task. The first service, translation, automatically obtains the different possible translations of an ontology tag by accessing different linguistic resources. This service also uses a composition method in order to translate compound tags (tags made up of multiple words). The second module, the ranking of translations, orders the different translations according to the similarity with their lexical and semantic context. The method is based on a measure of relationship based on glosses to the ambiguity of the translations. This is done by comparing the meanings associated with each possible translation and its context.
-Captures all the linguistic information associated with the concepts using a linguistic model as a repository (LIR). LIR, Linguistic Information Repository, is a portable model that can be associated with any of the terms of an OWL ontology through an OWL meta-ontology. The main classes that make up LIR (lexicalization, sense of definition, context of use, notes, and provenance) are organized around the LexicalEntry class, which is linked to each term in the ontology (through the hasLexicalEntry relation). The set of LIR concepts allows a complete description in natural language of the ontology term to which it is associated. Furthermore, by means of typical lexical relationships either in the same language (for example, hasSynonym) across languages ​​(hasTranslation), LIR organizes linguistic information in the same natural language and between different languages ​​in order to provide a set multilingual information that allows the localization of the ontology.
-Uses a synchronization mechanism to keep ontology and linguistic information synchronized. The addition of new terms in the ontology or the deletion of existing terms is controlled by the advanced change tracking (based on Delta1 resources) used in the NeOn Toolkit. This mechanism is capable of capturing changes even when ontologically they have changed their position in the ontology model. By adopting this feature, LabelTranslator can accurately identify the minimum set of changes required to adjust the structure of the linguistic model, a critical step to ensure that a major change is made to the localized ontology.
To download LabelTranslator, you need the NeOn Toolkit which can be downloaded from the main page of the NeOn Toolkit project. Once the tool has been downloaded and installed, through the update site provided by the tool, the LabelTranslator plug-in can be downloaded. The usage documentation is available in the tool once the plug-in has been downloaded and installed.",,,
Tecnologias y modelos,tecnologiasymodelos,LDP4j,"LDP4j es un framework de código abierto basado en Java para el desarrollo de aplicaciones de lectura y escritura de Linked Data basados en la especificación Linked Data Platform 1.0 (LDP) del W3C. LDP4j proporciona los componentes requeridos por los clientes y servidores para manejar sus comunicaciones basadas en LDP, ocultando la complejidad de los detalles del protocolo de los desarrolladores de aplicaciones y dejando que se centran en el desarrollo de su lógica de negocio específica para su aplicación.","LDP4j is an open source Java-based framework for developing Linked Data reading and writing applications based on the W3C's Linked Data Platform 1.0 (LDP) specification. LDP4j provides the components required by clients and servers to handle their LDP-based communications, hiding the complexity of protocol details from application developers and leaving them to focus on developing their specific business logic for their application.",http://www.ldp4j.org/,,
Tecnologias y modelos,tecnologiasymodelos,Lexical Model for ONtologies (lemon),"Lexical Model for ONtologies (lemon) es un modelo desarrollado colaborativamente en el proyecto Monnet y concebido para ser un estándar para el intercambio de información léxica en la Web Semántica. lemon se basa en trabajo anterior de miembros de Monnet como los modelos LexInfo, LIR, y LM. El modelo lemon pretende ser un modelo: conciso, descriptivo (no prescritivo), modular, y basado en RDF.","Lexical Model for ONtologies (lemon) is a model developed collaboratively in the Monnet project and conceived to be a standard for exchanging lexical information on the Semantic Web. lemon builds on previous work from Monnet members like the LexInfo, LIR, and LM models. The lemon model is intended to be a model: concise, descriptive (non-prescriptive), modular, and based on RDF.",http://lemon-model.net/,,
Tecnologias y modelos,tecnologiasymodelos,LIR - Linguistic Information Repository,,,,,
Tecnologias y modelos,tecnologiasymodelos,map4rdf,"Recientemente, hemos visto un gran incremento en la cantidad de datos geoespaciales que están publicados utilizando RDF y los principios de Linked Data. Esfuerzos como el W3C Geo XG, y más recientemente la iniciativa GeoSPARQL están proporcionando los vocabularios necesarios para publicar este tipo de información en la Web de Datos. map4rdf es un herramienta para explorar y visualizar conjuntos de datos RDF enriquecidos con información geométrica.
map4rdf es un software open source que simplemente necesita ser configurado para usar cualquier SPARQL endpoint y que proporciona a los usuarios una visualización de datos georreferenciados y en RDF en un mapa. Los aspectos geoespaciales de los datos se pueden modelar usando o el modelo de datos del W3C Geo XG o GeoSPARQL.","Recently, we have seen a large increase in the amount of geospatial data that is published using RDF and Linked Data principles. Efforts such as the W3C Geo XG, and more recently the GeoSPARQL initiative are providing the vocabularies necessary to publish this type of information on the Data Web. map4rdf is a tool for exploring and visualizing RDF data sets enriched with geometric information.
map4rdf is open source software that simply needs to be configured to use any SPARQL endpoint and that provides users with a visualization of georeferenced and RDF data on a map. The geospatial aspects of the data can be modeled using either the W3C Geo XG or GeoSPARQL data model.",http://oeg-upm.github.io/map4rdf,,https://github.com/oeg-upm/map4rdf
Tecnologias y modelos,tecnologiasymodelos,MARiMbA,"MARiMbA es una herramienta orientada a bibliotecas para transformar sus registros en formato MARC (MAchine-ReadableCataloging) a RDF, siguiendo las mejores prácticas de Linked Data.
La herramienta soporta todo el proceso de asignación de correspondencias transformación entre los metadatos contenidos en los registros MARC y los vocabularios elegidos para generar RDF. Es una herramienta diseñada para facilitar el proceso de generación de Linked Data y permitir que sea llevado a cabo por el personal de las bibliotecas sin necesidad de asistencia técnica. Para ello, MARiMbA ofrece las siguientes características:
-Está probada para transformar registros de autoridad y bibliográficos.
-Todo el trabajo se realiza a través de hojas de cálculo, no siendo necesario conocer el manejo de ningún lenguaje de mapeo o transformación adicional (XML, XSLT, etc.)
-La herramienta realiza un análisis previo de los registros a transformar, generando las plantillas para las correspondencias a partir de dicho análisis. Dichas plantillas están enfocadas a mejorar la toma de decisiones del usuario, la identificación de errores y la evaluación del proceso de transformación.
-Permite usar cualquier vocabulario o mezcla de vocabularios en RDFS/OWL
-Incluye un fichero de configuración que permite realizar ajustes en la transformación. En cualquier caso, viene con una configuración por defecto que sigue el modelo FRBR (Functional RequirementsforBibliographicRecords).
-Incluye un servidor SPARQL ligero (Fuseki) que permite al usuario ejecutar
La herramienta ha sido utilizada con éxito para realizar la transformación a RDF de cerca de 7 millones de registros MARC 21 de la Biblioteca Nacional de España, y que ha dado como resultado alrededor de 60 millones de tripletas RDF. Los datos transformados son accesibles a través de SPARQL en http://datos.bne.es/sparql. Por otro lado, un ejemplo de uno de los registros transformados es accesible en la siguiente dirección: http://datos.bne.es/resource/XX1718747

","MARiMbA is a tool aimed at libraries to transform their records in MARC (MAchine-ReadableCataloging) format to RDF, following the best practices of Linked Data.
The tool supports the entire transformation mapping process between the metadata contained in the MARC records and the vocabularies chosen to generate RDF. It is a tool designed to facilitate the Linked Data generation process and allow it to be carried out by library staff without the need for technical assistance. For this, MARiMbA offers the following features:
-It is proven to transform authority and bibliographic records.
-All the work is done through spreadsheets, not being necessary to know the handling of any additional mapping or transformation language (XML, XSLT, etc.)
-The tool performs a previous analysis of the records to be transformed, generating the templates for the matches from said analysis. These templates are focused on improving user decision-making, error identification and evaluation of the transformation process.
-Allows to use any vocabulary or vocabulary mix in RDFS / OWL
-Includes a configuration file that allows making adjustments to the transformation. In any case, it comes with a default configuration that follows the FRBR (Functional RequirementsforBibliographicRecords) model.
-Includes a lightweight SPARQL server (Fuseki) that allows the user to run
The tool has been used successfully to transform nearly 7 million MARC 21 records from the National Library of Spain to RDF, resulting in around 60 million RDF triples. The transformed data is accessible through SPARQL at http://datos.bne.es/sparql. On the other hand, an example of one of the transformed records is accessible at the following address: http://datos.bne.es/resource/XX1718747",,," ¿Cómo utilizarlo?
Necesitas:
-Registros MARC (autoridad y/o bibliográficos) en formato ISO 2709
-Java 1.6 o superior en el path (comprueba con java -version si no estás seguro)
-Un editor de hojas de cálculo (OpenOffice, LibreOffice, Ms Excel, etc.)
Pasos:
1 Guardar/mover los ficheros MARC a transformar en la carpeta data. Poner los bibliográficos en la carpeta data/bibliographic y los de autoridad en data/authority. Se pueden transformar tantos ficheros como se desee.
2 Ejecutar el comando que genera las plantillas de mapping: ""marimba --generatemappings -a -b""   Esta acción genera 3 hojas de cálculo: classificationMapping y annotationMapping, relationsMapping. Además, crea una hoja de cálculo adicional, alias, que permite asignar alias a aquellas clases y propiedades RDF más usadas para evitar utilizar la URI completa. Las hojas de cálculo se encuentran por defecto en la carpeta mappings.
3 Utilizando las hojas de cálculo generadas, asignar correspondencias entre las combinaciones encontradas en los registros MARC y las clases y propiedades de los vocabularios elegidos. Cada hoja de cálculo tiene una función definida:
-classificationMapping: asignar la clase o tipo de recurso RDF a generar para cada una de las combinaciones.
-annotationMapping: asignar la propiedad RDF a generar a partir de cada uno de los subcampos.
-relationsMapping: asignar la relación RDF a establecer entre los recursos encontrados que presentan una determinada variación de subcampos.
4 Guardar en la carpeta models los ficheros RDF de los vocabularios utilizados. Para ello necesitas descargarlos de la Web o exportarlos a un fichero si estabas usando un editor de ontologías (como NeOnToolkit o Protégé).
5 Ejecutar el comando que genera RDF tomando las correspondencias, los registros y los vocabularios: ""marimba --generaterdf -a -b --writeresultado.rdf""
6 Si se quieren hacer consultas SPARQL directamente sobre los datos se puede ejecutar un servidor RDF ligero (Fuseki). Para ello se debe ejecutar: ""run-marimba-server""
Y en la dirección http://localhost:3030/ se pueden hacer consultas sobre los datos generados"
Tecnologias y modelos,tecnologiasymodelos,morph,"El conjunto de tecnologías morph (junto con sus algoritmos correspondientes) se centra en aplicar una variedad de técnicas de reescritura de consultas sobre fuentes de datos federadas heterogéneas, mediante el uso de mapeos expresados ​​en el lenguaje W3C R2RML. La suite se compone de los siguientes idiomas:
- morph-RDB, para acceder a bases de datos relacionales. Actualmente brinda soporte para sistemas de administración de bases de datos relacionales como mySQL, Postgres y MonetDB. (https://oeg.fi.upm.es/index.php/es/technologies/315-morph-rdb/index.html)
- morph-LDP, una extensión de morph-RDB que funciona con nuestra implementación de Linked Data Platform [2]. (https://oeg.fi.upm.es/index.php/es/technologies/331-morph-ldp/index.html)
-morph-GFT, para acceder a Google Fusion Tables. (https://oeg.fi.upm.es/index.php/es/technologies/316-morph-gft/index.html)
-morph-streams, para acceder a los flujos de datos disponibles en Esper, GSN o SNEE, aunque extensible para otras fuentes de datos dinámicas que exponen datos a través de una API REST. (https://oeg.fi.upm.es/index.php/es/technologies/320-morph-streams/index.html)
-SPARQL-DQP, para acceder a puntos finales SPARQL federados. (https://oeg.fi.upm.es/index.php/es/technologies/166-sparql-dqp/index.html)
-kyrie, para enriquecer las consultas SPARQL considerando las implicaciones de ontologías. (https://oeg.fi.upm.es/index.php/es/technologies/314-kyrie/index.html)","The morph suite of technologies (together with their corresponding algorithms), is focused on applying a range of query rewriting techniques over heterogeneous federated data sources, through the use of mappings expressed in the W3C R2RML language. The suite is composed of the following languages:
- morph-RDB, for accessing relational databases. Currently it provides support for relational database management systems such as mySQL, Postgres and MonetDB. (https://oeg.fi.upm.es/index.php/es/technologies/315-morph-rdb/index.html)
- morph-LDP, an extension of morph-RDB that works with our Linked Data Platform implementation [2]. (https://oeg.fi.upm.es/index.php/es/technologies/331-morph-ldp/index.html)
-morph-GFT, for accessing Google Fusion Tables. (https://oeg.fi.upm.es/index.php/es/technologies/316-morph-gft/index.html)
-morph-streams, for accessing data streams available in Esper, GSN or SNEE, although extensible for other dynamic data sources that expose data through a REST API. (https://oeg.fi.upm.es/index.php/es/technologies/320-morph-streams/index.html)
-SPARQL-DQP, for accessing federated SPARQL endpoints. (https://oeg.fi.upm.es/index.php/es/technologies/166-sparql-dqp/index.html)
-kyrie, for enriching SPARQL queries by considering ontology entailments. (https://oeg.fi.upm.es/index.php/es/technologies/314-kyrie/index.html)",,,
Tecnologias y modelos,tecnologiasymodelos,morph-GFT,"morph-GFT es una extensión de morph-RDB que funciona con tablas de Google Fusion Table (GFT). Estas tablas se pueden describir mediante asignaciones R2RML y permite a los usuarios consultarlas mediante SPARQL. En morph-GFT, las consultas SPARQL planteadas por los usuarios se traducen en consultas similares a SQL que son compatibles con la API de GFT.
A diferencia de las implementaciones de bases de datos relacionales estándar que se utilizan normalmente con R2RML, la API de GFT no admite operaciones de unión. Por lo tanto, las consultas similares a SQL que se pueden evaluar directamente a través de la API de GFT son bastante simples. Por esta razón, utilizamos SPARQL-DQP [2] para realizar uniones de resultados intermedios y luego estos resultados intermedios se traducen utilizando las asignaciones R2RML especificadas por los usuarios. El uso de SPARQL-DQP aporta otro beneficio: no solo podemos unir los resultados intermedios de las tablas GFT, sino también de otros puntos finales SPARQL. [1] Aplicación de SPARQL-DQP para consultas SPARQL federadas a través de Google Fusion Tables, Freddy Priyatna, Carlos Buil Aranda, Oscar Corcho. Pista de demostración de Extended Semantic Web Conference (ESWC 2013). Enlace
[2] Semántica y optimización de la extensión de la federación SPARQL 1.1, Carlos Buil Aranda, Marcelo Arenas, Oscar Corcho. Extended Semantic Web Conference (ESWC2011), Semantic Data Management track, 2011. Premio al mejor artículo. Enlace","morph-GFT is an extension of morph-RDB that works with Google Fusion Table (GFT) tables. These tables can be described using R2RML Mappings and enables users to query them using SPARQL. In morph-GFT, SPARQL queries posed by users are translated into SQL-like queries that are supported by the GFT API.
Unlike standard relational database implementations normally used with R2RML, the GFT API does not support join operations. Therefore, the SQL-like queries that can be evaluated directly over the GFT API are rather simple. For this reason, we use SPARQL-DQP [2] to perform joins of intermediate results and then these intermediate results are translated using the R2RML mappings specified by the users. Using SPARQL-DQP brings another benefit: not only can we join the intermediate results from GFT tables, but also from other SPARQL endpoints.            [1] Applying SPARQL-DQP for Federated SPARQL Querying over Google Fusion Tables, Freddy Priyatna, Carlos Buil Aranda, Oscar Corcho. Extended Semantic Web Conference (ESWC 2013) demo track. link
[2] Semantics and optimization of the SPARQL 1.1 federation extension, Carlos Buil Aranda, Marcelo Arenas, Oscar Corcho. Extended Semantic Web Conference (ESWC2011), Semantic Data Management track, 2011. Best paper award. link",https://github.com/oeg-upm/morph-gft,https://oeg.fi.upm.es/images/architecture-morphgft.png,The morph-GFT project page is located at  https://github.com/oeg-upm/morph-gft  and the instructions of how to use morph-GFT can be found at  https://github.com/oeg-upm/morph-gft/wiki
Tecnologias y modelos,tecnologiasymodelos,morph-LDP,"morph-LDP [1] es una extensión de morph-RDB que funciona con nuestra implementación de Linked Data Platform [2]. morph-LDP expone los datos relacionales como datos vinculados de lectura / escritura para aplicaciones compatibles con LDP, al tiempo que permite que las aplicaciones heredadas continúen utilizando sus bases de datos relacionales. [1] Mihindukulasooriya, N., Priyatna F., Corcho, O., García-Castro, R. y Esteban-Gutiérrez, M. morph-LDP: implementación de una plataforma de datos enlazados basada en R2RML. La undécima sesión de demostración de la Conferencia de Web Semántica Extendida 2014, Creta, Grecia.
[2] Mihindukulasooriya, N., García-Castro, R., Esteban-Gutiérrez, M .: Linked Data Platform como un enfoque novedoso para la integración de aplicaciones empresariales. (Octubre de 2013)","morph-LDP[1] is an extension of morph-RDB that works with our Linked Data Platform implementation [2]. morph-LDP exposes relational data as read/write Linked Data for LDP-aware applications, whilst allowing legacy applications to continue using their relational databases.                                                                                                              [1] Mihindukulasooriya, N., Priyatna F., Corcho, O., García-Castro, R. and Esteban-Gutiérrez, M. morph-LDP: An R2RML-based Linked Data Platform implementation. The 11th Extended Semantic Web Conference 2014 Demo Session, Crete, Greece.
[2] Mihindukulasooriya, N., García-Castro, R., Esteban-Gutiérrez, M.: Linked Data Platform as a novel approach for Enterprise Application Integration. (Oct 2013)",https://oeg.fi.upm.es/images/stories/tecnologias/morph-ldp-scenario.png     https://oeg.fi.upm.es/images/stories/tecnologias/morph-ldp-1.png,,
Tecnologias y modelos,tecnologiasymodelos,morph-RDB,"morph-RDB [1,3] (anteriormente llamado ODEMapster) es un motor RDB2RDF desarrollado por Ontology Engineering Group, que sigue la especificación R2RML (http://www.w3.org/TR/r2rml/). morph-RDB admite dos modos operativos:
-Actualización de datos, que consiste en generar datos RDF a partir de una base de datos relacional según las descripciones del mapeo R2RML.
-Traducción de consultas, que permite evaluar consultas SPARQL sobre un conjunto de datos RDF virtual, reescribiendo esas consultas en SQL de acuerdo con las descripciones de mapeo de R2RML.
morph-RDB supera en rendimiento a herramientas similares de última generación (D2R) y algoritmos de traducción de consultas (por ejemplo, el propuesto por Chebotko y sus colegas en 2009) al emplear varios tipos de optimizaciones durante el proceso de reescritura de consultas, para generar más consultas SQL eficientes. Algunas de estas optimizaciones son la eliminación de autouniones, la eliminación de subconsultas y la eliminación de combinaciones externas izquierdas. morph-RDB ha sido probado con el benchmark sintético BSBM y se ha implementado con éxito en varios proyectos españoles / europeos (Integrate, Répener y BizkaiSense).
Por el momento, morph-RDB trabaja con sistemas de administración de bases de datos relacionales como MySQL, PostgreSQL y MonetDB. Además, morph-RDB también se ha ampliado para admitir Google Fusion Tables en un proyecto llamado morph-GFT [2] [6], y se ha integrado con nuestra implementación de Linked Data Platform (LDP) [4] [5].
El repositorio del proyecto morph-RDB se puede encontrar en https://github.com/oeg-upm/morph-rdb y las instrucciones de cómo usarlo se pueden encontrar en https://github.com/oeg-upm/morph -rdb / wiki.

[1] Freddy Priyatna, Oscar Corcho, Juan Sequeda. Formalización y experiencias de traducción de consultas de SPARQL a SQL basada en R2RML utilizando Morph. Conferencia World Wide Web (WWW 2014). Enlace
[2] Freddy Priyatna, Carlos Buil Aranda, Oscar Corcho. Aplicación de SPARQL-DQP para consultas SPARQL federadas a través de Google Fusion Tables. Pista de demostración de Extended Semantic Web Conference (ESWC 2013). Enlace
[3] Freddy Priyatna, Raúl Alonso-Calvo, Sergio Paraíso, Gueton Padrón-Sánchez y Oscar Corcho. Acceso basado en R2RML y consulta de datos clínicos relacionales con morph-RDB. Aplicaciones y herramientas web semántica para ciencias biológicas (SWAT 2015). (A aparecer)
[4] Mihindukulasooriya, N., García-Castro, R., Esteban-Gutiérrez, M .: La plataforma de datos enlazados como un enfoque novedoso para la integración de aplicaciones empresariales. (Octubre de 2013)
[5] Mihindukulasooriya, N., Priyatna F., Corcho, O., García-Castro, R. y Esteban-Gutiérrez, M. morph-LDP: implementación de una plataforma de datos enlazados basada en R2RML. Sesión de demostración de la 11ª Conferencia de Web Semántica Extendida 2014, Creta, Grecia.
[6] https://github.com/oeg-upm/morph-gft","morph-RDB [1,3] (formerly called ODEMapster) is an RDB2RDF engine developed by the Ontology Engineering Group, which follows the R2RML specification (http://www.w3.org/TR/r2rml/). morph-RDB supports two operational modes:
-Data upgrade, which consists in generating RDF data from a relational database according to the R2RML mapping descriptions.
-Query translation, which allows evaluating SPARQL queries over a virtual RDF dataset, by rewriting those queries into SQL according to the R2RML mapping descriptions.
morph-RDB outperforms similar state-of-the-art tools (D2R) and query translation algorithms (e.g., the one proposed by Chebotko and colleagues in 2009) by employing various types of optimisations during the query rewriting process, so as to generate more efficient SQL queries. Some of these optimisations are self-join elimination, subquery elimination, and left-outer join elimination. morph-RDB has been tested with the BSBM synthetic benchmark and has been successfully deployed in various Spanish/EU projects (Integrate, Répener, and BizkaiSense) .
At the moment, morph-RDB works with relational database management systems like MySQL, PostgreSQL and MonetDB. In addition, morph-RDB has also been extended to support Google Fusion Tables in a project called morph-GFT [2] [6], and has been integrated with our Linked Data Platform implementation (LDP) [4] [5].
The morph-RDB project repository can be found at https://github.com/oeg-upm/morph-rdb and the instructions of how to use it can be found at https://github.com/oeg-upm/morph-rdb/wiki.

[1] Freddy Priyatna, Oscar Corcho, Juan Sequeda. Formalisation and Experiences of R2RML-based SPARQL to SQL query translation using Morph. World Wide Web Conference (WWW 2014). link
[2] Freddy Priyatna, Carlos Buil Aranda, Oscar Corcho. Applying SPARQL-DQP for Federated SPARQL Querying over Google Fusion Tables. Extended Semantic Web Conference (ESWC 2013) demo track. link
[3] Freddy Priyatna, Raul Alonso-Calvo, Sergio Paraiso, Gueton Padron-Sanchez and Oscar Corcho. R2RML-based access and querying to relational clinical data with morph-RDB. Semantic Web Applications and Tools for Life Sciences (SWAT 2015). (To appear)
[4] Mihindukulasooriya, N., García-Castro, R., Esteban-Gutiérrez, M.: Linked Data Platform as a novel approach for Enterprise Application Integration. (Oct 2013)
[5] Mihindukulasooriya, N., Priyatna F., Corcho, O., García-Castro, R. and Esteban-Gutiérrez, M. morph-LDP: An R2RML-based Linked Data Platform implementation. The 11th Extended Semantic Web Conference 2014 Demo Session, Crete, Greece.
[6] https://github.com/oeg-upm/morph-gft",https://github.com/oeg-upm/morph-rdb,,
Tecnologias y modelos,tecnologiasymodelos,morph-streams,"morph-streams es un sistema de acceso a datos basado en ontología que permite evaluar consultas SPARQL-Stream en una variedad de sistemas de transmisión de datos, que se asignan utilizando el lenguaje W3C R2RML. Más específicamente, la versión actual de morph-streams proporciona envoltorios para:
-El motor de procesamiento de eventos complejos Esper.
-El middleware de la red de sensores GSN.
-El sistema de gestión de flujo de datos SNEE.
Las versiones anteriores de morph-streams también admitían la API de Pachube (ahora Xively), aunque ha quedado obsoleta.
Morph-streams admite dos modos de funcionamiento:
-Permite enviar consultas SPARQL-Stream directamente a una fuente de datos envuelta en R2RML. Las consultas se reescriben en el lenguaje de consulta subyacente o en la API REST y se envían al sistema subyacente, y los resultados se vuelven a traducir utilizando el mismo conjunto de asignaciones R2RML.
-Permite registrar consultas continuas SPARQL-Stream sobre una fuente de datos envuelta en R2RML, a la que los consumidores pueden suscribirse, recibiendo resultados actualizados tan pronto como son evaluados.
El repositorio del proyecto morph-streams se puede encontrar en https://github.com/oeg-upm/morph-streams, junto con instrucciones sobre cómo instalarlo y usarlo. Además, se puede encontrar una implementación en vivo de morph-streams con varios tipos de fuentes de datos de transmisión en http://streams.linkeddata.es/.","morph-streams is an ontology-based data access system that allows evaluating SPARQL-Stream queries over a range of data streaming systems, which are mapped using the W3C R2RML language. More specifically, the current version of morph-streams provides wrappers for:
-The complex event processing engine Esper.
-The sensor network middleware GSN.
-The data stream management system SNEE.
Previous versions of morph-streams also supported the API of Pachube (now Xively), although this has been deprecated.
Morph-streams supports two modes of operation:
-It allows submitting SPARQL-Stream queries directly to an R2RML-wrapped data source. Queries are rewritten into the underlying query language or REST API and submitted to the underlying system, and results are then translated back using the same set of R2RML mappings.
-It allows registering SPARQL-Stream continuous queries over an R2RML-wrapped data source, to which consumers can subscribe, receiving updated results as soon as they are evaluated.
The morph-streams project repository can be found at https://github.com/oeg-upm/morph-streams, together with instructions on how to install it and use it. Besides, a live deployment of morph-streams with several types of streaming data sources can be found at http://streams.linkeddata.es/.",https://github.com/oeg-upm/morph-streams,,"An efficient RDF processing engine for heterogeneous data streams
The purpose of this research is to design and implement an engine that allows complex queries over heterogeneous data streams in near real-time at Web scale.
There is a growing number of applications that depend on the usage of real-time spatiotemporal data, and which allow moving from the usual three levels of decision making (strategic, tactical, and operational) to real-time decision making. One example would be real-time geomarketing, where decisions on offering discount coupons to customers may be made on really short time slots based on the combination of a set of spatiotemporal data streams coming from different providers, e.g. public transport card validations or weather information. Extracting information from these streams is complex because of the heterogeneity of the data, the rate of data generation, and the volume. To tap these data sources accordingly and get relevant information, scalable processing infrastructures are required, as well as approaches to allow data integration and fusion.
Our plan is to build a distributed stream processing engine capable of adapting to changing conditions while serving complex continuous queries. First, adapters for various formats are used to convert heterogeneous streams to Linked Data streams. Then, Adaptive Query Processing (AQP) allows adjusting the query execution plan to varying conditions of the data input, the incoming queries, and the system.
Our engine will address real-time processing following the Lambda principles. Lambda is a 3-layer architecture designed to alleviate the complexities of Big Data management: a batch layer stores all the incoming data in an immutable master dataset and pre-computes batch views; a serving layer indexes views on the master dataset; and a speed layer manages the real-time processing issues and requests data views depending on incoming queries. We will follow this design together with AQP techniques and RDF compressed data structures allowing to decrease access time in large datasets, as well as data transmission time among processing nodes.

 "
Tecnologias y modelos,tecnologiasymodelos,OGSA-DAI RDF Resource,"OGSA-DAI RDF Resource es una extension al sistema de acceso a datos OGSA-DAI. Esta extensión extiende OGSA-DAI añadiéndole un nuevo recurso de datos a los ya existentes en el framework (bases de datos relacionales, XML y archivos). Este nuevo recurso accede a datos en RDF almacenados en una base de datos RDF o accesibles mediante SPARQL endpoints. El acceso se realiza mediante el framework RDF Jena y Jena SDB. En la siguiente figura se muestra la extensión dentro del contexto de OGSA-DAI. Esta imagen también muestra otra extensión a OGSA-DAI, la cual accede a un sistema RDB2RDF encargado de acceder a bases de datos relacionales mediante consultas a una ontología.El nuevo recurso RDF es el encargado de configurar el acceso a los repositorios RDF. El acceso real lo realizan actividades complementarias a él. Estas actividades acceden al recurso, el cual configura los reursos de datos. Las actividades consultan posteriormente los recursos de datos y procesan los resultados de las consultas. Estos resultados son convertidos al formato interno de OGSA-DAI. Este formato son tuplas de la forma (calor1, valor2,  …) y mediante este formato es posible integrar los resultados de otras consultas a otros recursos de datos de OGSA-DAI. El código del recurso puede descargarse del SVN de OGSA-DAI. Este recurso se utilice actualmente en el proyecto ADMIRE y también es utilizado por el Sciences and Technologies Facilities Council  en el Reino Unido.","OGSA-DAI RDF Resource is an extension to the OGSA-DAI data access system. This extension extends OGSA-DAI by adding a new data resource to the existing ones in the framework (relational databases, XML and files). This new resource accesses RDF data stored in an RDF database or accessible through SPARQL terminals. Access is done through the RDF Jena and Jena SDB frameworks. The following figure shows the extension within the context of OGSA-DAI. This image also shows another extension to OGSA-DAI, which accesses an RDB2RDF system in charge of accessing relational databases through queries to an ontology. The real access is carried out by activities complementary to it. These activities access the resource, which configures the data resources. The query activities then query the data resources and process the query results. These results are converted to the internal OGSA-DAI format. This format is tuples of the form (heat1, value2,  …) and in this format it is possible to integrate the results of other queries to other OGSA-DAI data resources. The resource code can be downloaded from the OGSA-DAI SVN. This resource is currently used in the ADMIRE project and is also used by the Sciences and Technologies Facilities Council in the UK.",http://sourceforge.net/apps/trac/ogsa-dai/,https://oeg.fi.upm.es/images/stories/tecnologias/RDFResource1.png                   https://oeg.fi.upm.es/images/stories/tecnologias/RDFResource2.png,
Tecnologias y modelos,tecnologiasymodelos,OOPS!  – OntOlogy Pitfall Scanner!,"OOPS! es una aplicación web, independiente de cualquier entorno de desarrollo de ontologías, para detectar malas prácticas en ontologías que podrían, potencialmente, provocar errores en el modelado de las mismas. El objetivo de esta herramienta es ayudar a los desarrolladores de ontologías durante la actividad de validación de las mismas, la cual puede dividirse en diagnóstico y reparación. Actualmente, OOPS! proporciona mecanismos para detectar automáticamente un número de errores potenciales, ayudando por tanto a los desarrolladores durante la actividad de diagnóstico.","OOPS! is a web application, independent of any ontology development environment, to detect bad practices in ontologies that could potentially cause errors in their modeling. The objective of this tool is to help ontology developers during their validation activity, which can be divided into diagnosis and repair. Currently, OOPS! provides mechanisms to automatically detect a number of potential bugs, thereby assisting developers during diagnostic activity.",http://oops.linkeddata.es/,https://oeg.fi.upm.es/images/stories/oops-1.png    https://oeg.fi.upm.es/images/stories/oops-2.png," ¿Cómo usar OOPS!?
1)    Accede a http://oops.linkeddata.es/
2)    Introduce la URI de la ontología a analizar o pega el código RDF en el cuadro de texto correspondiente (Ver Figura 1) y haz click en el botón asociado a cada opción.
3)    Comprueba los resultados obtenidos (Ver Figura 2).
Contacto: oops@delicias.dia.fi.upm.es

 "
Tecnologias y modelos,tecnologiasymodelos,Sem4Tags,"Sem4Tags es una herramienta multilingüe capaz de descubrir el significado de una etiqueta en un contexto dado y que como resultado ofrece un recurso DBpedia que define el significado de cada etiqueta procesada. La versión actual soporta etiquetas en inglés y español.
Sem4Tags identifica el significado de etiquetas multilingües asociándoles recursos DBpedia. Este proceso tiene en cuenta el contexto de la etiqueta entendido como el conjunto de las etiquetas del usuario que coocurren cuando anotan un recurso. Sem4Tags usa un repositorio de significados multilingüe (MSR por sus siglas en inglés)  creado a partir de información de la Wikipedia, donde cada palabra se relaciona con un conjunto de posibles significados que se han extraído de las páginas de desambiguación de Wikipedia.  Para seleccionar el significado más probable para una etiqueta, Sem4Tags utiliza un modelo de vectores que tiene en cuenta los términos que aparecen en cada una de las páginas Wikipedia que definen un significado.","Sem4Tags is a multilingual tool capable of discovering the meaning of a tag in a given context and that as a result offers a DBpedia resource that defines the meaning of each processed tag. The current version supports tags in English and Spanish.
Sem4Tags identifies the meaning of multilingual tags by associating DBpedia resources with them. This process takes into account the context of the label understood as the set of user labels that occur when they annotate a resource. Sem4Tags uses a multilingual meaning repository (MSR) created from Wikipedia information, where each word is related to a set of possible meanings that have been extracted from Wikipedia's disambiguation pages. To select the most likely meaning for a tag, Sem4Tags uses a vector model that takes into account the terms that appear on each of the Wikipedia pages that define a meaning.",http://grafias.dia.fi.upm.es/Sem4Tags/,,"Existe un servicio web disponible en la siguiente dirección:http://grafias.dia.fi.upm.es/SemanticTagWebServiceRestFul/resources/tag/disambiguate
Existe una aplicación on-line disponible en la siguiente dirección:http://grafias.dia.fi.upm.es/SemanticWebApp/DisambiguationTool.xhtml"
Tecnologias y modelos,tecnologiasymodelos,sitemap4rdf,"Sitemap4rdf es una herramienta de línea de comando que genera ficheros sitemap.xml para sitios de Linked Data que tienen un SPARQL endpoint. Sitemap4rdf consulta al endpoint y recupera una lista con todas las URLs, y genera el sitemap.xml, que debe ser subido al sitio web.
Entre sus características, incluye soporte a la compresión del Sitemap, y soporte para la partición del Sitemap en varios ficheros para sitios de gran tamaño.","Sitemap4rdf is a command line tool that generates sitemap.xml files for Linked Data sites that have a SPARQL endpoint. Sitemap4rdf queries the endpoint and retrieves a list with all the URLs, and generates the sitemap.xml, which must be uploaded to the website.
Among its features, it includes support for Sitemap compression, and support for partitioning the Sitemap into several files for large sites.",http://lab.linkeddata.deri.ie/2010/sitemap4rdf/,,
Tecnologias y modelos,tecnologiasymodelos,SPARQL-DQP,"Este sistema implementa la extensión para la federación de consultas de SPARQL 1.1, extendiendo el sistema OGSA-DAI/DQP. Además, optimiza la ejecución de las consultas, trabajo realizado por SPARQL-DQP. Para ello, primero se identifican una serie de patrones bien diseñados (descritos en [1] and [2]) y si dichos patrones son identificados el sistema utiliza una serie de reglas de escritura que mejoran el tiempo de ejecución de las consultas.

[1] J. Pérez, M. Arenas and C. Gutierrez, Semantics and complexity of SPARQL, TODS 34(3), 2009
[2] Semantics and optimization of the SPARQL 1.1 federation extension, Carlos Buil Aranda, Marcelo Arenas, Oscar Corcho. Extended Semantic Web Conference (ESWC2011), Semantic Data Management track, 2011. Best paper award. https://oeg.fi.upm.es/files/pdf/eswc11paper.pdf.
[3] Federating Queries in SPARQL1.1:Syntax, Semantics and Evaluation, Carlos Buil-Aranda, Marcelo Arenas, Oscar Corcho, Axel Polleres.http://www.websemanticsjournal.org/index.php/ps/article/view/321. ","This system implements the SPARQL 1.1 query federation extension, extending the OGSA-DAI / DQP system. In addition, it optimizes the execution of the queries, work carried out by SPARQL-DQP. To do this, first a series of well-designed patterns are identified (described in [1] and [2]) and if these patterns are identified, the system uses a series of writing rules that improve the execution time of the queries.
[1] J. Pérez, M. Arenas and C. Gutierrez, Semantics and complexity of SPARQL, TODS 34 (3), 2009
[2] Semantics and optimization of the SPARQL 1.1 federation extension, Carlos Buil Aranda, Marcelo Arenas, Oscar Corcho. Extended Semantic Web Conference (ESWC2011), Semantic Data Management track, 2011. Best paper award. https://oeg.fi.upm.es/files/pdf/eswc11paper.pdf.
[3] Federating Queries in SPARQL1.1: Syntax, Semantics and Evaluation, Carlos Buil-Aranda, Marcelo Arenas, Oscar Corcho, Axel Polleres.http: //www.websemanticsjournal.org/index.php/ps/article/view/ 321.",http://videolectures.net/eswc2011_corcho_extension/,,
Tecnologias y modelos,tecnologiasymodelos,vocab-express,"Hasta ahora, los principios y buenas prácticas de Linked Data están siendo adoptadas por un número creciente de proveedores de datos, teniendo como resultado una base de datos global en la Web que contiene cientos de conjuntos de datos LOD. En este contexto es importante promover la re-utilización y enlazado de conjuntos de datos, y para este fin, es necesario conocer la estructura de los conjuntos de datos. Un primer paso para conocer la estructura de un conjunto de datos es explorar el vocabulario utilizado por el conjunto de datos, y como el conjunto de datos está utilizando ese vocabulario.
vocab-express es una herramienta simple para explorar el vocabulario utilizado en un conjunto de datos dado. La herramienta provee toda la información relacionada del vocabulario:
1 la lista de todas las clases
2 la lista de todas las properties
3 el número de instancias de cada clase
4 el número de instancias de cada property
5 el lenguaje de las etiquetas y comentarios de los elementos del vocabulario.
vocab-express está siendo implementado en node.js, que es una plataforma construida sobre V8 (Google's open source JavaScript engine) para construir aplicaciones de red rápidas y escalables. La figura muestra el flujo de información de vocab-express.","So far, Linked Data principles and good practices are being adopted by a growing number of data providers, resulting in a global database on the Web containing hundreds of LOD data sets. In this context, it is important to promote the reuse and linking of data sets, and for this purpose, it is necessary to know the structure of the data sets. A first step in understanding the structure of a data set is to explore the vocabulary used by the data set, and how the data set is using that vocabulary.
vocab-express is a simple tool for exploring the vocabulary used in a given data set. The tool provides all the related vocabulary information:
1 the list of all classes
2 the list of all properties
3 the number of instances of each class
4 the number of instances of each property
5 the language of the labels and comments of the vocabulary elements.
vocab-express is being implemented in node.js, which is a platform built on V8 (Google's open source JavaScript engine) to build fast and scalable network applications. The figure shows the information flow of vocab-express.",,https://oeg.fi.upm.es/images/stories/tecnologias/vocab-express.jpg,
Tecnologias y modelos,tecnologiasymodelos,NOR2O,"NOR2O ya no está siendo utilizado, ni es mantenido por el grupo.

NOR2O es una librería para la transformación de recursos no ontológicos en ontologías. Los recursos no-ontológicos (NOR) son recursos de conocimiento cuya semántica no ha sido todavía formalizada en una ontología. Hay una gran cantidad de NORs que incorporan conocimientos sobre algunos dominios particulares y que representan algún grado de consenso.
En el contexto de la Metodología NeOn, proponemos un método general para la re-ingeniería de recursos no-ontológicos en ontologías. Se basa en  un patrón de re-ingeniería que define un procedimiento para transformar los componentes de un recurso no-ontológico en primitivas de una ontología, utilizando WordNet para desambiguar las relaciones implícitas de los componentes del NOR.
La figura 1 muestra el diagrama de arquitectura de alto nivel conceptual de los módulos implicados. El NOR connector carga los esquemas de clasificación, tesauros y lexicones con sus correspondientes modelos de datos e implementados en bases de datos, XML, ficheros de texto plano y hojas de cálculo..
El Transformer lleva a cabo la transformación propuesta por los patrones. Este módulo interactúa con el módulo Semantic Relation Disambiguator para obtener la semántica de las relaciones de los elementos NOR.
El Semantic Relation Disambiguator se encarga de obtener la semántica de las relaciones entre dos elementos NOR. Básicamente, el módulo recibe dos elementos NOR del módulo Transformer y devuelve la semántica entre ellos. El módulo conecta el recurso externo a través del módulo External Resource Service para obtener la relación.
El External Resource Service  se encarga de interactuar con los recursos externos para obtener la semántica de las relaciones entre elementos NOR. Actualmente el módulo interactúa con WordNet. Estamos implementando el acceso a DBpedia.
El OR Connector  genera la ontología en OWL Lite. Para ello, utiliza la OWL API. Finalmente, para concluir la descripción de la librería de software, conviene mencionar que la implementación de esta libraría sigue un enfoque modular, por lo tanto, es posible extenderla para incluir otros tipos de NOR, modelos de datos e implementaciones de una manera sencilla, así como la explotación de otros recursos externos para la desambiguación de relaciones.","NOR2O is no longer being used, nor is it maintained by the group.

NOR2O is a library for transforming non-ontological resources into ontologies. Non-ontological resources (NOR) are knowledge resources whose semantics have not yet been formalized in an ontology. There are a large number of NORs that incorporate knowledge about some particular domains and that represent some degree of consensus.
In the context of the NeOn Methodology, we propose a general method for the re-engineering of non-ontological resources in ontologies. It is based on a re-engineering pattern that defines a procedure to transform the components of a non-ontological resource into primitives of an ontology, using WordNet to disambiguate the implicit relationships of the NOR components.
Figure 1 shows the conceptual high-level architecture diagram of the modules involved. The NOR connector loads the classification schemes, thesauri and lexicons with their corresponding data models and implemented in databases, XML, plain text files and spreadsheets.
The Transformer carries out the transformation proposed by the patterns. This module interacts with the Semantic Relation Disambiguator module to obtain the semantics of the relationships of the NOR elements.
The Semantic Relation Disambiguator is responsible for obtaining the semantics of the relationships between two NOR elements. Basically the module receives two NOR elements from the Transformer module and returns the semantics between them. The module connects the external resource through the External Resource Service module to get the relationship.
The External Resource Service is responsible for interacting with external resources to obtain the semantics of the relationships between NOR elements. Currently the module interacts with WordNet. We are implementing access to DBpedia.
The OR Connector generates the ontology in OWL Lite. To do this, it uses the OWL API. Finally, to conclude the description of the software library, it should be mentioned that the implementation of this library follows a modular approach, therefore, it is possible to extend it to include other types of NOR, data models and implementations in a simple way, as well as the exploitation of other external resources for the disambiguation of relationships.",,https://oeg.fi.upm.es/images/stories/tecnologias/nor2o.png,
Tecnologias y modelos,tecnologiasymodelos,ODESeW,"ODESeW ya no está siendo utilizado ni mantenido por nuestro grupo.

ODESeW (Semantic Web Portal based on WebODE platform) es una aplicación basada en ontologías que genera automáticamente y gestiona portales de conocimiento para intranets y extranets. ODESeW está construido dentro de la plataforma de ingeniería de ontologías WebODE y provee las siguientes funcionalidades:
-Modela conocimiento mediante una plataforma de desarrollo de ontologías que entregra un conjunto de servicios de desarrollo de ontologías. Así como el portal de conocimiento se va a usar en la web, es altamente recomendado que use un servidor de ontologías (y no un editor de ontologías ejecutado de forma aislada) que permita construir ontologías de forma colaborativa así como dar acceso a los ontologías mediante internet. La construcción del portal sobre un servidor de ontologías facilita la sincronización de la información acorde a los cambios de las ontologías. Desde una perspectiva software, el portal se beneficia de los servicios presentes y futuros del servidor de ontologías.
-Edición e inserción de contenido mediante edición de instancias de ontologías. ODESeW permite insertar, actualizar y borrar instancias de clases, sus atributos y sus relaciones en una red de ontologías interrelacionadas y con diferentes permisos de edición para los usuarios del portal. Como parte de la edición de instancias, ODESeW puede ser utilizado como una herramienta de gestión documentacional en la que permite manejar documentos electrónicos.
-Visualización de contenido mediante visualizaciones para los usuarios de ontologías, relaciones e instancias altamente configurables. La ontologías es utilizada como índice de la información insertada y para navegar atreves de esta. El contenido almacenado en el portal puede ser accedido dinámicamente mediante minués generados por las ontologías acorde a los permisos del usuario, visualizando así las distintos tipos de información introducidos en el portal. El portal de conocimientos también provee anotación de su contenido en RDF(S), , DAML+OIL y OWL.
-Búsqueda y consultas del contenido basada en una aproximación híbrida entre ontologías y búsqueda por palabras. El modulo de búsquedas y consultas utiliza al interfaz de acceso de WebODE para acceder y consultar contenido de las ontologías.
-Servicios que facilitan la administración, que permiten gestionar los usuarios del portal, los permisos de edición y visualización, otros elementos de gestión del portal. Estos servicios puede ser solamente accedidos por aquellos usuarios que pertenecen al grupo de administración del portal.
-Como una ventaja importante de ODESeW frente a otros portales de conocimiento similares es la automática sincronización del portal y de las ontologías en las que se basa. Así, si una ontología es modificada dentro del editor de ontologías WebODE, los cambios se ven automáticamente reflejados en el portal, tanto como la conceptualización misma de la ontologías como de sus instancias.","ODESeW is no longer being used or maintained by our group.

ODESeW (Semantic Web Portal based on WebODE platform) is an ontology-based application that automatically generates and manages knowledge portals for intranets and extranets. ODESeW is built within the WebODE ontology engineering platform and provides the following functionalities:
-Models knowledge through an ontology development platform that delivers a set of ontology development services. Just as the knowledge portal will be used on the web, it is highly recommended that you use an ontology server (and not an ontology editor executed in isolation) that allows you to build ontologies in a collaborative way as well as give access to the ontologies through Internet. The construction of the portal on an ontology server facilitates the synchronization of the information according to the changes of the ontologies. From a software perspective, the portal benefits from the present and future services of the ontology server.
-Editing and inserting content by editing instances of ontologies. ODESeW allows inserting, updating and deleting class instances, their attributes and their relationships in a network of interrelated ontologies and with different editing permissions for portal users. As part of the edition of instances, ODESeW can be used as a document management tool in which it allows to manage electronic documents.
-Visualization of content through visualizations for users of highly configurable ontologies, relationships and instances. The ontologies is used as an index of the inserted information and to navigate through it. The content stored in the portal can be accessed dynamically through minutes generated by the ontologies according to the user's permissions, thus visualizing the different types of information entered in the portal. The knowledge portal also provides annotation of your content in RDF (S),, DAML + OIL and OWL.
-Search and content queries based on a hybrid approach between ontologies and word search. The search and query module uses the WebODE access interface to access and consult content from the ontologies.
-Services that facilitate administration, that allow managing portal users, editing and viewing permissions, other portal management elements. These services can only be accessed by those users who belong to the portal's administration group.
-As an important advantage of ODESeW compared to other similar knowledge portals is the automatic synchronization of the portal and the ontologies on which it is based. Thus, if an ontology is modified within the WebODE ontology editor, the changes are automatically reflected in the portal, as well as the very conceptualization of the ontologies and their instances.",,,
Tecnologias y modelos,tecnologiasymodelos,OWLDoc,"OWLDoc es un plug-in desarrollado dentro del proyecto NeOn. Este plug-in está implementado para la herramiento NeOn Toolkit y su función es añadir una opción al menú de exportación de la herramienta para generar documentos HTML a partir de una ontology OWL.
El plug-in utiliza el OWL API para extrar la información de la ontología OWL y crear una salida que contiene, de manera organizada, un conjunto de ficheros HTML que proporcionan la documentación acerca de la ontología y todos sus recursos.
Para descargar OWLDoc, es necesario disponer de NeOn Toolkit que se puede descargar desde la página principal del proyecto NeOn Toolkit. Una vez descargada la herramienta e instalada, mediante el update site que proporciona la herramienta, podrá descargarse el plug-in OWLDoc. La documentación de uso está disponible en la herramienta una vez descargado el plug-in e instalado.","OWLDoc is a plug-in developed within the NeOn project. This plug-in is implemented for the NeOn Toolkit tool and its function is to add an option to the tool's export menu to generate HTML documents from an OWL ontology.
The plug-in uses the OWL API to extract the information from the OWL ontology and create an output that contains, in an organized way, a set of HTML files that provide documentation about the ontology and all its resources.
To download OWLDoc, you need the NeOn Toolkit which can be downloaded from the NeOn Toolkit project home page. Once the tool has been downloaded and installed, through the update site provided by the tool, the OWLDoc plug-in can be downloaded. The usage documentation is available in the tool once the plug-in has been downloaded and installed.",,,
Tecnologias y modelos,tecnologiasymodelos,R2O y ODEMapster,"R2O & ODEMapster ya no está siendo utilizado, ni realizamos mantenimiento alguno en el grupo, puesto que ha sido reemplazado por el paquete de herramientas morph, que incluye morph-RDB, morph-GFT y morph-streams.
R2O & ODEMapster es un marco integrado para la expresión formal, la evaluación, verificación y explotación de correspondencias semánticas entre ontologías y bases de datos relacionales. El marco integrado está compuesto por:
-R2O, un lenguaje formal declarativo con expresividad suficiente como para representar situaciones de correspondencia complejas debidas al hecho de que se alinean dos modelos desarrollados y mantenidos de forma independiente y entre los que pueden darse disparidades de todo tipo.
-ODEMapster, procesador que se encarga del proceso de upgrade o enriquecimiento semántico del contenido de la base de datos o mediante la extracción bajo demanda del contenido de la base de datos en respuesta a preguntas planteadas en términos de la ontología mediante un proceso de re-escritura de consultas.
Por otro lado, ODEMapster plugin, desarrollado dentro el contexto del proyecto NeOn, proporciona una interfaz gráfica que permite crear, ejecutar o realizar consultas de ""mappings"" R2O. Este plugin está incuido en el NeOn Toolkit.
Para utilizar ODEMpaster plugin, primero hay que instalar NeOn Toolkit. NeOn Toolkit puede descargarse desde www.neon-toolkit.org. Una vez descargado, ejecute el fichero y siga las instrucciones.
Después de instalar NeOn Toolkit, el siguiente paso es instalar ODEMapster plugin. Este paso se puede realizar mediante las siguientes acciones:
1 Abrir Neon Toolkit.
2 Desde el menú Help de Neon Toolkit, elegir “Software Updates” --> “Find and Install”.
3 Una ventana Install/Update aparecerá en Neon Toolkit.
-Si ha instalado este plugin anteriormente, selecciones: “Search for update”
-Si no ha instalado este plugin anteriormente, seleccione: “Search for new features”
-Seleccione “Neon Toolkit Update Site” entre los posibles sitios de actualización y pulse finish
-Seleccione “ODEMapster” dentro de “Ontology Population” y presione next
-Seleccione “Agree” en el acuerdo de licencia.
-Pulse “Finish” en la ventana de instalación..
Para una información más completa, consulte el manual de ODEMapster. http://geo.linkeddata.es/c/document_library/get_file?uuid=2a944333-4cd6-4ff1-aaab-68f62fbf0d3c&groupId=10136
 ¿Cómo funciona?
Para la ontología hydrOntology y cada una de las bases de datos del IGN se creó un documento R2O que describe los mappings entre cada base de datos y la ontología. A continuación, se ejecutó el procesador ODEMapster para generar las instancias RDF.  Las bases de datos del IGN están almacenadas en MySQL y ORACLE e hydrOntology está en OWL.","R2O & ODEMapster is no longer being used, nor do we perform any maintenance on the group, since it has been replaced by the morph toolkit, which includes morph-RDB, morph-GFT and morph-streams.
R2O & ODEMapster is an integrated framework for the formal expression, evaluation, verification and exploitation of semantic correspondences between ontologies and relational databases. The integrated framework is made up of:
-R2O, a formal declarative language with enough expressiveness to represent complex correspondence situations due to the fact that two models developed and maintained independently are aligned and between which there can be disparities of all kinds.
-ODEMapster, processor that is in charge of the upgrade process or semantic enrichment of the content of the database or by means of the extraction on demand of the content of the database in response to questions posed in terms of the ontology through a process of re query writing.
On the other hand, ODEMapster plugin, developed within the context of the NeOn project, provides a graphical interface that allows creating, executing or making R2O mappings queries. This plugin is included in the NeOn Toolkit.
To use the ODEMpaster plugin, you must first install the NeOn Toolkit. NeOn Toolkit can be downloaded from www.neon-toolkit.org. Once downloaded, run the file and follow the instructions.
After installing the NeOn Toolkit, the next step is to install the ODEMapster plugin. This step can be done through the following actions:
1 Open Neon Toolkit.
2 From the Help menu of Neon Toolkit, choose “Software Updates” -> “Find and Install”.
3 An Install / Update window will appear in Neon Toolkit.
-If you have installed this plugin before, select: ""Search for update""
-If you have not installed this plugin before, select: ""Search for new features""
-Select “Neon Toolkit Update Site” among the possible update sites and press finish
-Select “ODEMapster” within “Ontology Population” and press next
-Select “Agree” in the license agreement.
-Press “Finish” in the installation window.
For more complete information, see the ODEMapster manual. http://geo.linkeddata.es/c/document_library/get_file?uuid=2a944333-4cd6-4ff1-aaab-68f62fbf0d3c&groupId=10136
 How does it work?
For the hydrOntology ontology and each of the IGN databases, an R2O document was created that describes the mappings between each database and the ontology. The ODEMapster processor was then run to generate the RDF instances. The IGN databases are stored in MySQL and ORACLE and hydrOntology is in OWL.",,https://oeg.fi.upm.es/images/stories/odemapsterprocess.png   https://oeg.fi.upm.es/images/stories/igndatasets.png,"Lista de usuarios de ODEMapster
FAO - http://fao.org
Guntars Bumars - University of Latvia - Institute of Mathematics and Computer Science - http://www.lumii.lv/Pages/computer.htm
Christian M. Fletcher - Department of Computer Science - Durham University - http://www.dur.ac.uk/ecs/computing.science/undergraduate09/whatis/
iSOCO - www.isoco.com
GIS4GOV project
SemSorGrid4Env - http://www.semsorgrid4env.eu/
GeoLinkedData - http://geo.linkeddata.es/
WEB n+1 - http://www.webenemasuno.es"
Tecnologias y modelos,tecnologiasymodelos,WebODE,"WebODE ya no está siendo utilizado ni mantenido por nuestro grupo desde 2006.

WebODE es un suite extensible de ingeniería de ontologías basada en un servidor de aplicaciones que empezó en 1999. El núcleo de WebODE es su servicio de acceso a sus ontologías, utilizado por todos los demás servicios y aplicación acopladas al servidor. El editor de ontologías WebODE permite editar y navegar por las ontologías, y esta basada en formularios HTML y applets de Java.

WebODE fue construido como una plataforma escalable, extensible e integrada que cubre y da soporte a la mayoría de las actividades involucradas en el proceso de desarrollo de ontologías (conceptualización, razonamiento, intercambio, etc.) y provee un conjunto de servicios relaciones a las ontologías que permite interoperar con otros sistemas de información. Entre estos servicios, la plataforma integra servicios para la importación y exportación de lenguajes de ontologías (XML, RDF(S), OIL, DAML+OIL, OWL, CARIN, FLogic, Jess, Prolog), para la edición de axiomas mediante WAB (WebODE Axiom Builder), para generar documentación, para evaluar, para controlar la evolución, para extraer ontologías, para juntarlas y un motor de inferencia.

La publicación más relevante es Most relevant publications:

WebODE in a nutshell. AI Magazine 2003.","WebODE is no longer being used or maintained by our group since 2006.

WebODE is an extensible application server-based ontology engineering suite that began in 1999. The core of WebODE is your ontology access service, used by all other server-coupled applications and services. The WebODE ontology editor allows editing and browsing of ontologies, and is based on HTML forms and Java applets.

WebODE was built as a scalable, extensible and integrated platform that covers and supports most of the activities involved in the ontology development process (conceptualization, reasoning, exchange, etc.) and provides a set of services related to ontologies that allows interoperation with other information systems. Among these services, the platform integrates services for the import and export of ontology languages ​​(XML, RDF (S), OIL, DAML + OIL, OWL, CARIN, FLogic, Jess, Prolog), for the edition of axioms using WAB ( WebODE Axiom Builder), to generate documentation, to evaluate, to control evolution, to extract ontologies, to put them together and an inference engine.

The most relevant publication is Most relevant publications:

WebODE in a nutshell. AI Magazine 2003.",,https://oeg.fi.upm.es/images/stories/tecnologias/webode.png,
Tecnologias y modelos,tecnologiasymodelos,WS-DAIOnt-RDF(S),"WS-DAIOnt-RDF(S) ya no está siendo utilizado ni mantenido por nuestro grupo.

WS-DAIOnt-RDF(S) es una especificación que define un mecanismo de acceso a ontologías RDF(S) utilizando una aproximación orientada a servicios. Esta especificación, compatible con la arquitectura OGSA, define el conjunto de recursos de datos, mensajes e interfaces de acceso necesarios para la integración de ontologías en RDF (S) en cualquier aplicación (Grid) orientada a servicios.  RDF(S) Grid Access Bridge  (RGAB) es la implementación de referencia de dicha especificación. Esta implementación desacopla el mecanismo de acceso de la ubicación física real de las ontologías RDF(S) así como del sistema de almacenamiento utilizado para su persistencia.
De esta forma, las ontologías RDF (S) pueden ser almacenadas en un conjunto distribuido de sistemas de almacenamiento de RDF (S) y hacerse accesibles mediante los mecanismos de acceso definidos por la especificación WS-DAIOnt-RDF (S).","WS-DAIOnt-RDF (S) is no longer being used or maintained by our group.

WS-DAIOnt-RDF (S) is a specification that defines an access mechanism to RDF (S) ontologies using a service-oriented approach. This specification, compatible with the OGSA architecture, defines the set of data resources, messages and access interfaces necessary for the integration of ontologies in RDF (S) in any service-oriented application (Grid). RDF (S) Grid Access Bridge (RGAB) is the reference implementation of that specification. This implementation decouples the access mechanism from the actual physical location of the RDF (S) ontologies as well as from the storage system used for their persistence.
In this way, RDF (S) ontologies can be stored in a distributed set of RDF (S) storage systems and made accessible through the access mechanisms defined by the WS-DAIOnt-RDF (S) specification.",,,
Servicios,servicios,esDBpedia,"DBpedia ocupa un lugar central  en la Web de Datos por el importante volumen de datos semánticos que proporciona a partir de la información contenida en los infoboxes (Fichas en español) de la Wikipedia del idioma inglés. La última versión de DBpedia (v3.9, septiembre de 2013) proporciona datos semánticos de 4 millones de entidades, de las que 3.2 millones están clasificadas conforme a la ontología DBpedia: Personas (832 mil),  Lugares (639 mil), Discos musicales (116 mil), y un largo etcétera (529 claes y 2333 propiedades).
A partir de los 30 millones de artículos en 240 idiomas que almacena Wikipedia (4.4 millones en inglés, 1.1 millones en español), DBpedia genera 2460 millones datos semánticos (tripletas RDF) en 119 idiomas (470 millones en inglés, y 100 millones en español).
Hasta julio de 2011 (versiones DBpedia anteriores a la v3.7), sólo se extraía información de una página en español de la Wikipedia si se apuntaba a ella desde la versión inglesa. Por tanto, muchas páginas con información local relevante (e.g. pueblos, ríos, organizaciones) a la que no se podía llegar desde la versión inglesa de Wikipedia, quedaban fuera del proceso de extracción de DBpedia. En julio de 2011, la versión 3.7 de DBpedia se construyó usando un nuevo mecanismo de extracción que rompía con la dependencia de la versión inglesa de Wikipedia, permitiendo generar, adicionalmente, información semántica a partir de las versiones de Wikipedia en 15 idiomas, entre ellos el español (ca, de, el, es , fr, ga, hr, hu, it, nl, pl, pt, ru, sl, tr). La versión 3.8 de DBpedia (junio 2012) amplió la lista de idiomas a 111, y la versión actual (3.9) llegó a los 119 idiomas.

Jornadas de mapeos
Sin embargo, para que la Web de Datos se enriquezca con la información contenida en la versión española de la Wikipedia, es necesario un esfuerzo colectivo importante a fin de aumentar el número de mapeos de Fichas. DBpedia pone a disposición de la comunidad herramientas web para editar estos mapeos en los que se indica la correspondencia entre términos de las Fichas y los términos de la ontología de DBpedia. La I Jornada esDBpedia  logró mapear el 80% de los datos de la wikipedia del idioma español y permitió la creación de es.dbpedia.org, el capítulo español de DBpedia en mayo de 2012. La II edición de las jornadas (diciembre de 2012) se centró en mejorar la calidad de los mapeos.","DBpedia occupies a central place in the Data Web due to the important volume of semantic data that it provides from the information contained in the infoboxes (Tabs in Spanish) of the English-language Wikipedia. The latest version of DBpedia (v3.9, September 2013) provides semantic data for 4 million entities, of which 3.2 million are classified according to the DBpedia ontology: People (832 thousand), Places (639 thousand), Music records (116 thousand), and a long etcetera (529 classes and 2333 properties).
From the 30 million articles in 240 languages ​​that Wikipedia stores (4.4 million in English, 1.1 million in Spanish), DBpedia generates 2.46 billion semantic data (RDF triples) in 119 languages ​​(470 million in English, and 100 million in Spanish).
Until July 2011 (DBpedia versions prior to v3.7), information was only extracted from a Spanish Wikipedia page if it was pointed to from the English version. Therefore, many pages with relevant local information (e.g. towns, rivers, organizations) that could not be reached from the English version of Wikipedia, were left out of the DBpedia extraction process. In July 2011, version 3.7 of DBpedia was built using a new extraction mechanism that broke the dependency on the English version of Wikipedia, allowing the generation of semantic information from Wikipedia versions in 15 languages, including Spanish (ca, de, el, es, fr, ga, hr, hu, it, nl, pl, pt, ru, sl, tr). DBpedia version 3.8 (June 2012) expanded the list of languages ​​to 111, and the current version (3.9) reached 119 languages.

Mapping days
However, for the Web of Data to be enriched with the information contained in the Spanish version of Wikipedia, a significant collective effort is necessary in order to increase the number of file mappings. DBpedia makes available to the community web tools to edit these mappings in which the correspondence between the terms of the Cards and the terms of the DBpedia ontology is indicated. The 1st esDBpedia Conference managed to map 80% of the wikipedia data for the Spanish language and allowed the creation of es.dbpedia.org, the Spanish chapter of DBpedia in May 2012. The 2nd edition of the conference ( December 2012) focused on improving the quality of the mappings.",,,
Metodologi­as,metodologias,La Metodología NeOn,"La Metodología NeOn para la construcción de redes de ontologías es una metodología basada en escenarios que se apoya en los aspectos de colaboración de desarrollo de ontologías y la reutilización, así como en la evolución dinámica de las redes de ontologías en entornos distribuidos. Las claves de la Metodología NeOn son:
-Un conjunto de nueve escenarios para la construcción de ontologías y redes de ontologías, haciendo hincapié en la reutilización de los recursos ontológicos y no ontológicos, la reingeniería y la fusión, y teniendo en cuenta la colaboración y el dinamismo.
-El Glosario de Procesos y Actividades identifica y define aquellos procesos y actividades involucrados en el desarrollo de redes de ontologías.
-Directrices metodológicas para diferentes procesos y actividades del proceso de desarrollo de la ontología de la red, tales como la reutilización y la reingeniería de los recursos ontológicos y no ontológicos, la especificación de los requisitos de la ontología, la localización de la ontología, la programación, etc. Todos los procesos y actividades se describen con (a) una tarjeta llena, (b) un flujo de trabajo, y (c) ejemplos.

Escenario 1: Desde la especificación de la aplicación. La red de ontologías se desarrolla a partir de cero (sin volver a utilizar los recursos existentes). Los desarrolladores deben especificar los requisitos de la ontología (enlace a las directrices). Después de eso, se asesora para llevar a cabo una búsqueda de recursos potenciales para ser reutilizados. A continuación, la actividad de planificación se debe realizar (enlace a las directrices), y los desarrolladores deben seguir el plan.
Escenario 2: La reutilización y reingeniería de los recursos no ontológicosc (NOR). Los desarrolladores deben llevar a cabo el proceso de reutilización NOR para decidir, de acuerdo con los requisitos de la ontología, que NORs pueden ser reutilizados para construir la red de la ontología. A continuación, los NORs seleccionados deben ser volver al proceso de re-ingeniería ontológicas. Las directrices se presentan en el enlace de directrices.
Escenario 3: La reutilización de los recursos ontológicos. Los desarrolladores utilizan recursos ontológicos (ontologías como un conjunto de módulos ontológicos (enlace a las directrices enlace 1, enlace 2), y / o declaraciones ontológicas (enlace a las directrices)) para construir redes de ontologías.
Escenario 4: La reutilización y re-ingeniería de los recursos ontológicos. Los desarrolladores de ontologías reutilizan los recursos y reorganizar los recursos  ontológicos.
Escenario 5: La reutilización y la fusión de los recursos ontológicos. Este escenario se produce cuando varios recursos ontológicos en el mismo dominio que se seleccionan para su reutilización, y los desarrolladores desean crear un nuevo recurso ontológico con los recursos seleccionados. Las directrices se presentan en el enlace de directrices.
Escenario 6: Reutilización, la fusión y re-ingeniería de los recursos ontológicos. Los desarrolladores de ontologías reutilizan, combinan y reorganizan los recursos-ontológicos. Este escenario es similar al Escenario 5, pero en este caso los desarrolladores deciden reorganizar el conjunto de recursos combinados.
Escenario 7: Reutilización de los patrones de diseño de ontologías (ODPs). Los desarrolladores de ontologías acceden a repositorios de reutilización ODPs.
Escenario 8: Reestructuración de recursos ontológicos. Los desarrolladores de ontologías reestructuran (modularizan, podan, extienden y / o especializan) recursos ontológicos que deben integrarse posteriormente en la red de ontologías.
Escenario 9: Localización de recursos ontológicos. Los desarrolladores de ontologías adaptan una ontología a otras lenguas y la cultura las comunidades, obteniendo así una ontología multilingüe. Las directrices se presentan en el enlace de directrices.","The NeOn Methodology for the construction of ontology networks is a scenario-based methodology that relies on the collaborative aspects of ontology development and reuse, as well as the dynamic evolution of ontology networks in distributed environments. The keys to the NeOn Methodology are:
-A set of nine scenarios for the construction of ontologies and ontology networks, emphasizing the reuse of ontological and non-ontological resources, reengineering and fusion, and taking into account collaboration and dynamism.
-The Glossary of Processes and Activities identifies and defines those processes and activities involved in the development of ontology networks.
-Methodological guidelines for different processes and activities of the network ontology development process, such as reuse and reengineering of ontological and non-ontological resources, specification of ontology requirements, ontology localization, programming, etc. All processes and activities are described with (a) a filled card, (b) a workflow, and (c) examples.

Scenario 1: From the application specification. The ontology network is developed from scratch (without reusing existing resources). Developers must specify the ontology requirements (link to guidelines). After that, they are advised to carry out a search for potential resources to be reused. Next, the planning activity should be done (link to guidelines), and the developers should follow the plan.
Scenario 2: The reuse and reengineering of non-ontological resourcesc (NOR). Developers must carry out the NOR reuse process to decide, according to the ontology requirements, which NORs can be reused to build the ontology network. Next, the selected NORs must be returned to the ontological reengineering process. The guidelines are presented in the guidelines link.
Scenario 3: The reuse of ontological resources. Developers use ontological resources (ontologies as a set of ontological modules (link to guidelines link 1, link 2), and / or ontological statements (link to guidelines)) to build networks of ontologies.
Scenario 4: The reuse and re-engineering of ontological resources. Ontology developers reuse resources and reorganize ontology resources.
Scenario 5: The reuse and fusion of ontological resources. This scenario occurs when multiple ontological resources in the same domain are selected for reuse, and the developers want to create a new ontological resource with the selected resources. The guidelines are presented in the guidelines link.
Scenario 6: Reuse, fusion and re-engineering of ontological resources. Ontology developers reuse, combine, and rearrange ontological-resources. This scenario is similar to Scenario 5, but in this case the developers decide to reorganize the pool of combined resources.
Scenario 7: Reuse of ontology design patterns (ODPs). Ontology developers access repositories for reuse ODPs.
Scenario 8: Restructuring of ontological resources. Ontology developers restructure (modularize, prune, extend and / or specialize) ontological resources that must later be integrated into the ontology network.
Scenario 9: Location of ontological resources. Ontology developers adapt an ontology to other languages ​​and culture communities, thus obtaining a multilingual ontology. The guidelines are presented in the guidelines link.",http://oa.upm.es/3879/,https://oeg.fi.upm.es/images/stories/tecnologias/neonmethodology.png,
Linked data,linkeddata,LinkedData.es,"LinkedData(.es) es una sitio web en el que se pueden consultar, de una manera rápida, las iniciativas, proyectos, colaboraciones y aplicaciones desarrolladas por el grupo en el ámbito de los Datos Enlazados. Como se puede observar, el grupo ha desarrollado y sigue desarrollando trabajos en diferentes dominios (geográfico, cultural, datos abiertos etc) tanto con instituciones públicas (BNE, RTVE, IGN, etc) como con empresas privadas (Grupo PRISA). También nos encargamos de la DBpedia en español, esDBpedia. ","LinkedData (.es) is a website where you can quickly consult the initiatives, projects, collaborations and applications developed by the group in the field of Linked Data. As can be seen, the group has developed and continues to develop work in different domains (geographic, cultural, open data, etc.) both with public institutions (BNE, RTVE, IGN, etc.) and with private companies (Grupo PRISA). We also take care of the DBpedia in Spanish, esDBpedia.",http://linkeddata.es/,,
Linked data,linkeddata,GeoLinked Data,"GeoLinked Data (.es) es una iniciativa para el enriquecimiento de la Web de los Datos con datos geoespaciales del territorio nacional español. Esta iniciativa se puso en marcha en el año 2008 con la publicación de diversas fuentes de información geográfica procedentes del Instituto Geográfico Nacional español, haciéndolas disponibles como bases de conocimiento RDF (Resource Description Framework) conforme a los principios de Linked Data.
De esta manera, España se sumaba en dicho momento a la iniciativa que otros países como el Reino Unido habían comenzado también.
Algunos de los productos que se obtuvieron como resultado de este trabajo fueron los siguientes:
-geometry2rdf, una librería para la generación de RDF geográfico. Esta librería fue utilizada como base para la generación de la librería TripleGeo, que se ha utilizado para la generación de RDF geográfico de un gran número de fuentes de datos geográficas europeas.
-map4RDF, para la visualización de datos geográficos enlazados, disponibles en un punto de acceso SPARQL.
Desde entonces, desde nuestro grupo hemos continuado trabajando en la mejora de estas herramientas, con el objetivo de ofrecer un conjunto de sistemas que puedan ser utilizados para la generación de RDF geográfico, de acuerdo con las lecciones aprendidas por la comunidad internacional en estos primeros pasos, así como teniendo en cuenta los principales resultados del grupo de trabajo del W3C sobre Spatial Data on the Web.
Así, hemos trabajado en crear un plugin para GeoKettle que hace uso y extiende la librería TripleGeo, y en estos momentos (año 2017), estamos trabajando en la reedición de una nueva versión del portal de datos geográficos y de Linked Data Geográfico, basado en la base de datos BTN100, que incorpora todos estos avances realizados en los últimos años. La descripción de este portal se actualizará una vez que se haya desplegado esta nueva versión a finales del año 2017.","GeoLinked Data (.es) is an initiative to enrich the Data Web with geospatial data from the Spanish national territory. This initiative was launched in 2008 with the publication of various sources of geographic information from the Spanish National Geographic Institute, making them available as RDF (Resource Description Framework) knowledge bases in accordance with Linked Data principles.
In this way, Spain joined at that time the initiative that other countries such as the United Kingdom had also started.
Some of the products that were obtained as a result of this work were the following:
-geometry2rdf, a library for generating geographic RDFs. This library was used as the basis for the generation of the TripleGeo library, which has been used for the generation of geographic RDFs from a large number of European geographic data sources.
-map4RDF, for displaying linked geographic data, available on a SPARQL access point.
Since then, our group has continued to work on improving these tools, with the aim of offering a set of systems that can be used for the generation of geographic RDF, in accordance with the lessons learned by the international community in these first steps. , as well as taking into account the main results of the W3C working group on Spatial Data on the Web.
Thus, we have worked on creating a plugin for GeoKettle that makes use of and extends the TripleGeo library, and at this time (year 2017), we are working on the reissue of a new version of the geographic data portal and Linked Geographic Data, based on the BTN100 database, which incorporates all these advances made in recent years. The description of this portal will be updated once this new version has been deployed at the end of 2017.",,,
Material utilizado en artículos,materialarticulos,Detecting common scientific workflow fragments using templates and execution provenance,"La evaluación del artículo ""Detecting common scientific workflow fragments using templates and execution provenance"" aceptado en K-CAP 2013 se encuentra disponible en el siguiente link. Los datos expuestos consisten en los grafos que se han utilizado como entrada para el experimento (derivados del SPARQL endpoint público de Wings), los resultados y logs obtenidos por el algoritmo SUBDUE durante su ejecución y algunas estadísticas y hojas excel resumiendo dichos resultados (por ejemplo, destacando el  número de estructuras que son independientes, contando aquellas que son relevantes, etc.).

Si está interesado en obtener más información puede leer el siguiente artículo: Daniel Garijo, Oscar Corcho y Yolanda Gil. Detecting common scientific workflow fragments using templates and execution provenance. K-CAp 2013, Banf, Canada.","The evaluation of the article ""Detecting common scientific workflow fragments using templates and execution provenance"" accepted in K-CAP 2013 is available at the following link. The exposed data consist of the graphs that have been used as input for the experiment (derived from the SPARQL public endpoint of Wings), the results and logs obtained by the SUBDUE algorithm during its execution and some statistics and excel sheets summarizing said results (for example , highlighting the number of structures that are independent, counting those that are relevant, etc.).

If you are interested in obtaining more information, you can read the following article: Daniel Garijo, Oscar Corcho and Yolanda Gil. Detecting common scientific workflow fragments using templates and execution provenance. K-CAp 2013, Banf, Canada.",https://www.oeg-upm.net/files/dgarijo/kcap2013Eval,,
Material utilizado en artículos,materialarticulos,Semantic Grounding of Tags (sem4tags),"Se publican los datos de la evaluación de las asociaciones de entidades semánticas producidas por sem4tags y sus variaciones para las etiquetas extraídas de Flickr (link a conjunto de datos).  Las evaluaciones están en un archivo CSV donde los diferentes campos que componen la evaluación están separados por el carácter “| ”. Cada fila en este archivo representa una evaluación individual de la asociación de una entidad semántica a una etiqueta. Recuerde que cada asociación semántica fue evaluada por al menos tres evaluadores. Finalmente, se recomienda leer el archivo readme donde se explica en detalle cada uno de los atributos que describen cada evaluación. Para cualquier duda acerca de este conjunto de datos se puede contactar a Andrés García-Silva","The data of the evaluation of the associations of semantic entities produced by sem4tags and their variations for the tags extracted from Flickr (link to dataset) are published. The evaluations are in a CSV file where the different fields that make up the evaluation are separated by the character  “|”. Each row in this file represents an individual evaluation of the association of a semantic entity to a label. Remember that each semantic association was evaluated by at least three evaluators. Finally, it is recommended to read the readme file where each of the attributes that describe each evaluation is explained in detail. For any questions about this data set, you can contact Andrés García-Silva",,,
Material utilizado en artículos,materialarticulos,Characterising Emergent Semantics in Twitter Lists,"El siguiente conjunto de datos contiene los datos de las listas de Twitter que han sido utilizados en los experimentos presentados en el artículo ""Characterising Emergent Semantics in Twitter Lists"" del 9th Extended Semantic Web Conference (ESWC 2012). Los datos se encuentran en una archivo de respaldo de una base de datos mysql version 5.5.14 que contiene información de las listas y de los distintos tipos de usuarios que interactúan con ellas. Además hemos incluido un diagrama entidad relación que representan las tablas que contienen la información.
Por favor si usted usa este conjunto de datos cite la siguiente referencia: García-Silva A., Kang J.H., Lerman K and Corcho O. Characterising Emergent Semantics in Twitter Lists. In proceedings of the 9th extended Semantic Web Conference ESWC, Crete, 2012.
Para preguntas sobre el conjunto de dato, contacte con: Andrés García-Silva","The following dataset contains the data from the Twitter lists that have been used in the experiments presented in the article ""Characterizing Emergent Semantics in Twitter Lists"" of the 9th Extended Semantic Web Conference (ESWC 2012). The data is in a backup file of a mysql version 5.5.14 database that contains information about the lists and the different types of users who interact with them. We have also included an entity relationship diagram that represents the tables that contain the information.
If you use this data set, please cite the following reference: García-Silva A., Kang J.H., Lerman K and Corcho O. Characterizing Emergent Semantics in Twitter Lists. In proceedings of the 9th extended Semantic Web Conference ESWC, Crete, 2012.
For questions about the data set, contact: Andrés García-Silva",,,
Material utilizado en artículos,materialarticulos,Efficient Inference-aware RDB2RDF Query Rewriting,"Esta página proporciona acceso a los ficheros para el artículo ""Inference-aware RDB2RDF Query Rewriting"". Los enlaces son estos:
-Casos de prueba de la FAO.https://oeg.fi.upm.es/delicias.dia.fi.upm.es/~jmora/qr4rdb2rdf/fao.zip
-Comparación entre modos de funcionamiento en el algoritmo. https://oeg.fi.upm.es/delicias.dia.fi.upm.es/~jmora/qr4rdb2rdf/tests-table4.zip
Explicación de los contenidos
Casos de prueba de la FAO
Los contenidos del archivo zip son un conjunto de casos de prueba, cada uno separado en un directorio distinto. Para cada caso de prueba se pueden encontrar los siguientes contenidos:
-El fichero de la ontología, necesario para la inferencia.
-El fichero de los mappings, especificado en R2O.
-El fichero de los mappings expresado en una forma más esquemática, para los lectores que no estén familiarizados con R2O.
-Un fichero de comparación, que contiene las diferencias entre la ontología y el fichero de los mappings. El artículo especifica el número de conceptos, object properties y datatype properties en la ontología, el número de los mismos que están cubiertos y también la cobertura como un porcentaje. La información presentada aquí está menos procesada y es más detallada. En estos ficheros se enumeran los elementos que están presentes en sólo uno de los dos ficheros, lo que permite comprobar los elementos específicos que causan la cobertura parcial de la ontología por parte de los mappings y posiblemente también viceversa.
Comparación de los modos de funcionamiento
Los contenidos del fichero zip están agrupados por el fichero de mappings utilizado para probar el algoritmo, ése es el significado de los cuatro directorios, atlas, bcn and egm para los ficheros de mappings Atlas, BCN200 y EGM que se corresponden con Hydrontology, l directorio phenom para Phenomenontology y el único fichero de mappings considerado para la evaluación con esta ontología en el artículo En cada uno de estos directorios los contenidos son equivalentes:
-Un fichero de ontología, Hydrontology para atlas, bcn y egm. Phenomenontology para phenom.
-Un fichero de mappings dependiendo de el directorio, uno entre Atlas, BCN200 y EGM o los mappings para Phenomenontology.
-Un fichero en texto plano con la consulta que se realiza al sistema, siempre nombrada como ""queries.txt"", la consulta es ""Q(?0) <- Aguas(?0)"" para los ejemplos con Hydrontology y ""Q(?0) <- Red(?0)"" para el ejemplo con Phenomenontology.
-Un conjunto de ficheros de texto conteniendo los resultados de la ejecución. Estos ficheros se corresponden con dos patrones diferentes:
-resXY.txt donde X es el modo de funcionamiento correspondiente con el sistema REQUIEM original e Y es el modo de funcionamiento correspondiente con los métodos de poda desarrollados en el contexto del artículo actual. Estos ficheros contienen la traza de ejecución con todas las cláusulas generadas en cada fase del algoritmo. Debido a la verbosidad para generar esta traza los tiempos de ejecución pueden variar ligeramente al compararlos con los tiempos descritos en el artículo.
-resRQMXY-0 donde X es el modo de funcionamiento correspondiente con el sistema REQUIEM original e Y es el modo de funcionamiento correspondiente con los métodos de poda desarrollados en el contexto del artículo actual. Los contenidos del fichero muestran la reescritura final obtenida además de información adicional referente al caso de prueba correspondiente.","This page provides access to the files for the article ""Inference-aware RDB2RDF Query Rewriting"". The links are these:
-FAO test cases.https: //oeg.fi.upm.es/delicias.dia.fi.upm.es/~jmora/qr4rdb2rdf/fao.zip
-Comparison between operating modes in the algorithm. https://oeg.fi.upm.es/delicias.dia.fi.upm.es/~jmora/qr4rdb2rdf/tests-table4.zip
Explanation of the contents
FAO test cases
The contents of the zip file are a set of test cases, each separated in a different directory. The following contents can be found for each test case:
-The ontology file, necessary for the inference.
-The mappings file, specified in R2O.
-The mappings file expressed in a more schematic way, for readers who are not familiar with R2O.
-A comparison file, which contains the differences between the ontology and the mappings file. The article specifies the number of concepts, object properties and datatype properties in the ontology, the number of them that are covered and also the coverage as a percentage. The information presented here is less processed and more detailed. These files list the elements that are present in only one of the two files, which allows checking the specific elements that cause partial coverage of the ontology by the mappings and possibly also vice versa.
Comparison of operating modes
The contents of the zip file are grouped by the mappings file used to test the algorithm, that is the meaning of the four directories, atlas, bcn and egm for the Atlas, BCN200 and EGM mappings files that correspond to Hydrontology, l directory phenom for Phenomenontology and the only mappings file considered for the evaluation with this ontology in the article In each of these directories the contents are equivalent:
-An ontology file, Hydrontology for atlas, bcn and egm. Phenomenontology for phenom.
-A mappings file depending on the directory, one between Atlas, BCN200 and EGM or the mappings for Phenomenontology.
-A plain text file with the query that is made to the system, always named as ""queries.txt"", the query is ""Q (? 0) <- Aguas (? 0)"" for the examples with Hydrontology and ""Q ( ? 0) <- Red (? 0) ""for the example with Phenomenontology.
-A set of text files containing the results of the execution. These files correspond to two different patterns:
-resXY.txt where X is the operating mode corresponding to the original REQUIEM system and Y is the operating mode corresponding to the pruning methods developed in the context of the current article. These files contain the execution trace with all the clauses generated in each phase of the algorithm. Due to the verbosity to generate this trace, the execution times may vary slightly when compared to the times described in the article.
-resRQMXY-0 where X is the operating mode corresponding to the original REQUIEM system and Y is the operating mode corresponding to the pruning methods developed in the context of the current article. The contents of the file show the final rewrite obtained as well as additional information regarding the corresponding test case.",,,
Material utilizado en artículos,materialarticulos,Collaborative Ontology Editing Framework - Usability Survey,"Encuesta online en la siguiente página.https://www.oeg-upm.net/files/UsabilitySurvey/survey.htm

Guías, ficheros de configuración y resultados disponibles en el siguiente enlace.https://www.oeg-upm.net/files/material/experimentData.zip

Tesis Doctoral disponible en el siguiente enlace.https://www.oeg-upm.net/files/material/dissertation-RAP-Digital.pdf","Online survey on the following page: https://www.oeg-upm.net/files/UsabilitySurvey/survey.htm

Guides, configuration files and results available at the following link: https://www.oeg-upm.net/files/material/experimentData.zip

Doctoral thesis available at the following link: https://www.oeg-upm.net/files/material/dissertation-RAP-Digital.pdf",,,
Benchmarks,benchmarks,Federated SPARQL query benchmark,"En el grupo OEG hemos diseñado un benchmark para evaluar la nueva funcionalidad para federar consultas SPARQL propuesta por el grupo del W3C, SPARQL-WG. Este benchmark utiliza consultas que son enviadas a varios SPARQL endpoints del proyecto Bio2RDF . Estas consultas han sido diseñadas en conjunto con expertos en biología del mismo proyecto, incrementando en complejidad una query básica, cubriendo así un mayor rango de consultas posibles.

Estas consultas primero acceden al SPARQL endpoint gene id, el cual contiene un conjunto de identificadores de genes. Estos identificadores los utilizamos en conjunto con los identificadores de los genes del repositorio Pubmed para obtener información acerca de unos descriptores. Estos descriptores pertenecen a la taxonomía MeSH, la cual es la taxonomía base del National Library of Medicine de los EEUU. Una vez tenemos esa taxonomía podemos completar la información he hemos recogido de esos 3 SPARQL endpoints y completarla con la información del endpoint HHPID el cual provee interacciones entre genes y el virus HIV.
Ficheros disponibles:
-https://oeg.fi.upm.es/files/sparql-dqp/mesh.sdb.n3.zip
-https://oeg.fi.upm.es/files/sparql-dqp/ncbi.gene2pubmed.n3.zip
-https://oeg.fi.upm.es/files/sparql-dqp/pubmedai.zip
-https://oeg.fi.upm.es/files/sparql-dqp/HHPID.tar
Semantics and optimization of the SPARQL 1.1 federation extension, Carlos Buil Aranda, Marcelo Arenas, Oscar Corcho. To appear in Extended Semantic Web Conference (ESWC2011), Semantic Data Management track, 2011.","In the OEG group we have designed a benchmark to evaluate the new functionality to federate SPARQL queries proposed by the W3C group, SPARQL-WG. This benchmark uses queries that are sent to various SPARQL endpoints of the Bio2RDF project. These queries have been designed in conjunction with biology experts from the same project, increasing the complexity of a basic query, thus covering a greater range of possible queries.

These queries first access the SPARQL endpoint gene id, which contains a set of gene identifiers. We use these identifiers together with the gene identifiers from the Pubmed repository to obtain information about some descriptors. These descriptors belong to the MeSH taxonomy, which is the base taxonomy of the US National Library of Medicine. Once we have that taxonomy we can complete the information we have collected from those 3 SPARQL endpoints and complete it with the information from the HHPID endpoint which provides interactions between genes and the HIV virus.
Files available:
-https: //oeg.fi.upm.es/files/sparql-dqp/mesh.sdb.n3.zip
-https: //oeg.fi.upm.es/files/sparql-dqp/ncbi.gene2pubmed.n3.zip
-https: //oeg.fi.upm.es/files/sparql-dqp/pubmedai.zip
-https: //oeg.fi.upm.es/files/sparql-dqp/HHPID.tar
Semantics and optimization of the SPARQL 1.1 federation extension, Carlos Buil Aranda, Marcelo Arenas, Oscar Corcho. To appear in Extended Semantic Web Conference (ESWC2011), Semantic Data Management track, 2011.",,,